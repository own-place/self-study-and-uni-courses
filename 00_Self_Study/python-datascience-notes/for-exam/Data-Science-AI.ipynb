{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0e9685",
   "metadata": {},
   "source": [
    "# Grading\n",
    "**Written test(individual) - 60%**\n",
    "<br>\n",
    "This written test of 40 multiple choice questions with 4 possible answers is about all the theory presented in the lectures. There will be no Python-specific questions. The test will contain multiple choice questions. The questions are designed to apply the knowledge obtained from the lectures to test your understanding.\n",
    "<br>\n",
    "\n",
    "**Project report(group) - 40%**\n",
    "<br>\n",
    "1. The actual content of the final report is 20 pages maximum. The appendix may be 20 pages maximum. The content counts from the start of your introduction to the last sentence of your last section (conclusion). \n",
    "2. Follow the guidelines in the Writing Guidelines regarding appendices, bibliography and formatting.\n",
    "3. Only use screenshots of your graphs. Make proper tables for the other components of data understanding (e.g. descriptive statistics, formulas). That is to say â€“ no screenshots of code output. \n",
    "4. If you want to show some code to provide evidence towards fulfilling the LOs, put your code in an appendix. You may either make separate entries within that appendix with the relevant code snippets, or put your code in its entirety in one appendix and refer to the relevant lines within the text. Put the code in monospace font to discern it from regular text. Use the Word plugin for code to do this. This means also not putting in screenshots of your code.\n",
    "5. You take responsibility for paraphrasing correctly (or quoting if necessary) and the ethical use of sources, including accurate source referencing following APA guidelines. Use the standard Word functionality under References for this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a234b",
   "metadata": {},
   "source": [
    "## 1. Introduction to AI\n",
    "### What do we want to achieve with AI?\n",
    "- **Now**: Narrow AI ç‹­ä¹‰äººå·¥æ™ºèƒ½\n",
    "  - Automate processes è‡ªåŠ¨åŒ–æµç¨‹\n",
    "  - Improve decision-making æ”¹è¿›å†³ç­–è¿‡ç¨‹\n",
    "  - Build productivity tools æ„å»ºæå‡æ•ˆç‡çš„å·¥å…·\n",
    "- **Later**: AGI & Superintelligence é€šç”¨äººå·¥æ™ºèƒ½å’Œè¶…äººå·¥æ™ºèƒ½\n",
    "  - Perform any task a human can èƒ½å®Œæˆä»»ä½•äººç±»èƒ½åšçš„ä»»åŠ¡\n",
    "  - Solve novel problems èƒ½è§£å†³ä»æœªé‡åˆ°çš„æ–°é—®é¢˜\n",
    "  - Show creativity and common sense å±•ç°åˆ›é€ åŠ›ä¸å¸¸è¯†æ¨ç†èƒ½åŠ›\n",
    "\n",
    "### Narrow AI (Machine Learning)\n",
    "- Works with **available data** ä½¿ç”¨**å·²æœ‰æ•°æ®**è¿›è¡Œå·¥ä½œ\n",
    "- Focused on **specific tasks** ä¸“æ³¨äº**ç‰¹å®šä»»åŠ¡**\n",
    "- Core functions: \n",
    "  - Pattern recognitionæ¨¡å¼è¯†åˆ«ï¼šRecommendation systemsæ¨èç³»ç»Ÿ, facial recognitionäººè„¸è¯†åˆ«\n",
    "  - Predictioné¢„æµ‹ï¼šmaintenanceç»´æŠ¤, demand forecastingéœ€æ±‚é¢„æµ‹\n",
    "  - Content generationå†…å®¹ç”Ÿæˆï¼šChatGPT, Copilot, DALLÂ·E\n",
    "\n",
    "### What makes Narrow AI work?\n",
    "- **Main ingredients**:\n",
    "  - Data æ•°æ®\n",
    "  - Model of the data distribution å¯¹æ•°æ®åˆ†å¸ƒçš„å»ºæ¨¡\n",
    "  - Probability & inferential statistics æ¦‚ç‡ä¸æ¨ç†ç»Ÿè®¡æ–¹æ³•\n",
    "  - Model tuning æ¨¡å‹è°ƒä¼˜\n",
    "\n",
    "### AGI (Artificial General Intelligence)\n",
    "| Narrow AI | AGI |\n",
    "|-----------|-----|\n",
    "| Task-specificç‰¹å®šä»»åŠ¡ | Cross-domainè·¨é¢†åŸŸä»»åŠ¡ |\n",
    "| Based on past dataåŸºäºå†å²æ•°æ® | Can handle noveltyèƒ½å¤„ç†æ–°æƒ…å†µ |\n",
    "| Pattern recognitionæ¨¡å¼è¯†åˆ« | Pattern interpretation & reasoningæ¨¡å¼è§£é‡Šä¸æ¨ç† |\n",
    "| Increases efficiencyæé«˜æ•ˆç‡ | Matches/exceeds human flexibilityåŒ¹æ•Œæˆ–è¶…è¶Šäººç±»çš„çµæ´»æ€§ |\n",
    "\n",
    "<br>\n",
    "\n",
    "> Adding complexity or data â‰  more intelligence å¢åŠ å¤æ‚æ€§æˆ–æ•°æ® â‰  å¢å¼ºæ™ºèƒ½\n",
    "> Intelligence requires reasoning beyond seen data æ™ºèƒ½éœ€è¦è¶…è¶Šå·²çŸ¥æ•°æ®çš„æ¨ç†èƒ½åŠ›\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Business Understanding (CRISP-DM Phase)\n",
    "### Key Questions\n",
    "- **Business Objective**: What is the organization trying to achieve? ç»„ç»‡è¯•å›¾å®ç°ä»€ä¹ˆï¼Ÿ\n",
    "- **Business Success Criteria**: When is that considered successful? ä»€ä¹ˆæƒ…å†µä¸‹è®¤ä¸ºç›®æ ‡è¾¾æˆï¼Ÿï¼ˆæ˜ç¡®KPIï¼‰\n",
    "- **Data Mining Goal**: How can data science help? æ•°æ®ç§‘å­¦å¦‚ä½•æä¾›å¸®åŠ©ï¼Ÿ\n",
    "- **Data Mining Success Criteria**: When is the data science effort successful? å¦‚ä½•åˆ¤æ–­æ•°æ®ç§‘å­¦å·¥ä½œæ˜¯å¦æˆåŠŸï¼Ÿï¼ˆæ˜ç¡®å‡†ç¡®ç‡æˆ–RMSEç­‰ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Evaluation & Baseline Models\n",
    "### Why use a baseline model?\n",
    "- Understand your data performance äº†è§£æ•°æ®çš„åŸºæœ¬è¡¨ç°\n",
    "- Identify data or modeling issues å‘ç°æ•°æ®æˆ–å»ºæ¨¡ä¸­å­˜åœ¨çš„é—®é¢˜\n",
    "- Faster iterations with simpler models ä½¿ç”¨ç®€å•æ¨¡å‹å¯ä»¥æ›´å¿«é€Ÿåœ°è¿­ä»£\n",
    "- Interpretable results for stakeholders æä¾›å¯è§£é‡Šçš„ç»“æœç»™ç›¸å…³æ–¹å‚è€ƒ\n",
    "- Provide a fair benchmark for advanced models ä¸ºé«˜çº§æ¨¡å‹æä¾›ä¸€ä¸ªå…¬å¹³çš„å¯¹æ¯”åŸºå‡†\n",
    "\n",
    "### How to define Data Mining Success Criteria?\n",
    "Use one or more of the following methods:\n",
    "- **Relative improvement over baseline ç›¸å¯¹æå‡**: \"15% improvement over mean prediction\" æ¯”å‡å€¼é¢„æµ‹æå‡ 15%\n",
    "- **Business impact as a metric ä¸šåŠ¡å½±å“æŒ‡æ ‡**: \"10% waste reduction = â‚¬500 savings/month\" å‡å°‘ 10% æµªè´¹ = æ¯æœˆèŠ‚çœ 500 æ¬§å…ƒ\n",
    "- **Industry/regulatory benchmarks è¡Œä¸šæˆ–æ³•è§„æ ‡å‡†**: \"False negatives < 5% when predicting disease\" ç–¾ç—…é¢„æµ‹ä¸­æ¼æŠ¥ç‡ < 5%\n",
    "- **Statistical significance ç»Ÿè®¡æ˜¾è‘—æ€§æ ‡å‡†**: \"Improve math scores by +6 points to be beyond Â±3 point fluctuation\" æ•°å­¦æˆç»©æé«˜ 6 åˆ†ä»¥è¶…å‡º Â±3 çš„è‡ªç„¶æ³¢åŠ¨èŒƒå›´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fda30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Data Understanding  \n",
    "## 4.1 Preliminary Data Inspection åˆæ­¥æ•°æ®æ£€æŸ¥\n",
    "- Data types and structure (e.g., numerical, categorical) æ•°æ®ç±»å‹ä¸ç»“æ„ï¼ˆæ•°å€¼å‹ã€åˆ†ç±»å‹ç­‰ï¼‰  \n",
    "- Variable distributions (check for normality) å˜é‡åˆ†å¸ƒï¼ˆæ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼‰  \n",
    "- Correlations between variables å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§  \n",
    "- Missing values ç¼ºå¤±å€¼  \n",
    "- Is the dataset suitable for modeling? What are the potential issues? æ˜¯å¦é€‚åˆå»ºæ¨¡ï¼Ÿæœ‰å“ªäº›æ½œåœ¨é—®é¢˜ï¼Ÿ  \n",
    "\n",
    "## 4.2 Distributions  \n",
    "**Purpose**: Understand the shape, skewness, and normality of variables.  \n",
    "ç›®çš„ï¼šäº†è§£å˜é‡çš„å½¢çŠ¶ã€åæ€ã€æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒ  \n",
    "\n",
    "**Visualization methods**:  \n",
    "- Histogram ç›´æ–¹å›¾: shows frequency of numerical variables æ•°å€¼å‹å˜é‡é¢‘ç‡   \n",
    "- Bar plot æŸ±çŠ¶å›¾: compares grouped categorical values åˆ†ç»„ï¼ˆåˆ†ç±»å˜é‡ï¼‰ä¸å…¶æ•°å€¼æ¯”è¾ƒ   \n",
    "- Boxplot ç®±çº¿å›¾: shows central tendency, distribution, and outliers  é›†ä¸­è¶‹åŠ¿ã€åˆ†å¸ƒã€ç¦»ç¾¤å€¼ \n",
    "\n",
    "Statistical tests like **t-test** and **ANOVA** can be used to test for normality. å¯ä»¥ä½¿ç”¨ t æ£€éªŒã€ANOVA æ¥æŸ¥çœ‹å˜é‡æ˜¯å¦æœä»æ­£æ€åˆ†å¸ƒã€‚ <br> \n",
    "If the distribution is not normal, apply **log transformation**: suitable for right-skewed data, reduces tail length and outlier influence. å¦‚æœä¸æ˜¯ï¼Œé‡‡ç”¨å¯¹æ•°å˜æ¢ï¼ˆlog transformï¼‰ï¼šé€‚ç”¨äºå³åå˜é‡ï¼Œæ‹‰è¿‘å°¾éƒ¨ï¼Œå‡å°‘æç«¯å€¼å½±å“ã€‚<br>   \n",
    "Alternatively, consider using **non-linear models**. æˆ–è€…ä½¿ç”¨éçº¿æ€§æ¨¡å‹ã€‚  \n",
    "\n",
    "## 4.3 Outlier Detection  \n",
    "**Purpose**: Identify and handle extreme values that may bias the model. è¯†åˆ«å’Œå¤„ç†å¼‚å¸¸ç‚¹ï¼Œé¿å…å…¶å¯¹æ¨¡å‹äº§ç”Ÿä¸å½“å½±å“  \n",
    "\n",
    "**Detection methods**:  \n",
    "- **Boxplot method**:  \n",
    "  - Lower bound = Q1 - 1.5 Ã— IQR  \n",
    "  - Upper bound = Q3 + 1.5 Ã— IQR  \n",
    "- **Z-score method**:  \n",
    "  - Z = (x - mean) / standard deviation  \n",
    "  - Z < -3 or Z > 3 is considered an outlier  \n",
    "\n",
    "**Handling strategies**:  \n",
    "- Investigate cause: data entry error or device malfunction?  è°ƒæŸ¥åŸå› ï¼šæ˜¯å¦ä¸ºå½•å…¥é”™è¯¯æˆ–è®¾å¤‡å¼‚å¸¸  \n",
    "- Delete: only if clearly erroneous  åˆ é™¤ï¼šä»…é™äºæ˜ç¡®ä¸ºé”™è¯¯æ•°æ®  \n",
    "- Retain: if it's a meaningful extreme value  ä¿ç•™ï¼šå¦‚æœæ˜¯æœ‰æ„ä¹‰çš„æç«¯å€¼  \n",
    "- Important: document all changes to avoid introducing bias  é‡è¦ï¼šè®°å½•æ‰€æœ‰ä¿®æ”¹æ“ä½œï¼Œé¿å…å¼•å…¥åå·®  \n",
    "\n",
    "Outlier â‰  Error â€” always investigate the reason.  ç¦»ç¾¤å€¼ â‰  é”™è¯¯ï¼Œä¸€å®šè¦è°ƒæŸ¥åŸå›   \n",
    "\n",
    "## 4.4 Feature Scaling  \n",
    "**Purpose**: Bring variables to a similar scale for better convergence and fair influence in models. è®©å˜é‡åœ¨ç›¸ä¼¼å°ºåº¦ä¸Šï¼Œæœ‰åˆ©äºå»ºæ¨¡æ”¶æ•›ã€é¿å…æŸå˜é‡ä¸»å¯¼æ¨¡å‹  \n",
    "\n",
    "**Common scaling methods**:\n",
    "\n",
    "| Method           | Description æè¿°                                 | Suitable scenarios é€‚ç”¨åœºæ™¯                           |\n",
    "|------------------|--------------------------------------------------|------------------------------------------------------|\n",
    "| StandardScaler   | Mean = 0, Std = 1 (Z-score standardization)      | Linear models, neural networks, gradient descent     |\n",
    "|                  | å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ï¼ˆZ-score æ ‡å‡†åŒ–ï¼‰             | çº¿æ€§æ¨¡å‹ã€ç¥ç»ç½‘ç»œã€æ¢¯åº¦ä¸‹é™ç±»ç®—æ³•                    |\n",
    "| MinMaxScaler     | Rescales values to [0, 1] range                  | Image processing, interpretable scales               |\n",
    "|                  | æ‰€æœ‰å€¼å‹ç¼©åˆ° [0,1] åŒºé—´                           | å›¾åƒå¤„ç†ã€éœ€è¦æ¢å¤ä¸šåŠ¡å•ä½çš„æ¨¡å‹                      |\n",
    "| RobustScaler     | Based on IQR, resistant to outliers              | Data with many outliers                              |\n",
    "|                  | åŸºäº IQRï¼ŒæŠ—ç¦»ç¾¤ç‚¹                                | æ•°æ®æœ‰å¤§é‡ç¦»ç¾¤å€¼æ—¶                                   |\n",
    "\n",
    "**Note**:  \n",
    "- Always scale based on **training set only** to avoid data leakage.  ç¼©æ”¾åº”ä»…åŸºäºè®­ç»ƒé›†ï¼Œä»¥é˜²æ•°æ®æ³„éœ²  \n",
    "- Do **not** scale binary variables or categorical variables.  ä¸è¦å¯¹äºŒå…ƒå˜é‡æˆ–åˆ†ç±»å˜é‡ç¼©æ”¾  \n",
    "- Scaling is **not needed** for tree-based models.  ä¸ç”¨äºæ ‘æ¨¡å‹  \n",
    "\n",
    "## 4.5 Correlation & Multicollinearity  \n",
    "**Purpose**: Understand relationships between variables and identify redundancy. ç†è§£å˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œåˆ¤æ–­æ¨¡å‹è¾“å…¥æ˜¯å¦å†—ä½™  \n",
    "\n",
    "**Correlation metrics**:\n",
    "\n",
    "| Metric           | Description                                  | Notes                                  |\n",
    "|------------------|----------------------------------------------|----------------------------------------|\n",
    "| Pearson's r      | Measures linear correlation [-1, 1] çº¿æ€§ç›¸å…³æ€§ï¼Œå€¼åœ¨ [-1, 1] ä¹‹é—´ | Most common; for continuous variables æœ€å¸¸ç”¨ï¼Œé€‚ç”¨äºæ•°å€¼å‹å˜é‡ |\n",
    "| Spearman's Ï     | Measures monotonic rank correlation å•è°ƒå…³ç³»çš„ç§©ç›¸å…³ï¼Œé€‚ç”¨äºæ’åºæ•°æ® | Captures non-linear monotonic patterns å¯æ•æ‰éçº¿æ€§å•è°ƒå…³ç³» |\n",
    "| Distance corr.   | Captures all types of dependencies è¡¡é‡æ‰€æœ‰ç±»å‹çš„ç›¸å…³æ€§ï¼ˆéçº¿æ€§ç­‰ï¼‰| Powerful but harder to interpret æ›´å¼ºä½†ä¸æ˜“è§£é‡Š |\n",
    "\n",
    "**Tip**: Always use scatter plots to visually confirm correlation.  æ€»æ˜¯å¯è§†åŒ–ï¼ˆå¦‚æ•£ç‚¹å›¾ï¼‰æ¥éªŒè¯ç›¸å…³æ€§æ˜¯å¦åˆç†ã€‚  \n",
    "\n",
    "### 4.5.1 Multicollinearity  \n",
    "**Problems**:  \n",
    "- Unstable coefficients, difficult to interpret  ç³»æ•°ä¸ç¨³å®šï¼Œè§£é‡Šå›°éš¾  \n",
    "- Poor generalization  æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™  \n",
    "\n",
    "**Detection**:  \n",
    "- Correlation heatmap  ç›¸å…³ç³»æ•°çƒ­å›¾  \n",
    "- Variance Inflation Factor (VIF) æ–¹å·®è†¨èƒ€å› å­: VIF > 5 or 10 indicates high multicollinearity\n",
    "\n",
    "**Solutions**:  \n",
    "- Drop one variable (choose less relevant or meaningful)  åˆ é™¤å…¶ä¸­ä¸€ä¸ªå˜é‡ï¼ˆé€‰æ‹©æ›´å¼±ç›¸å…³ã€å«ä¹‰æ›´å¼±è€…ï¼‰  \n",
    "- Combine variables (e.g., average or use PCA)  åˆå¹¶å˜é‡ï¼ˆä¾‹å¦‚å¹³å‡å€¼ã€ä¸»æˆåˆ†åˆ†æç­‰ï¼‰  \n",
    "- Apply regularization methods (e.g., Ridge or Lasso regression)  ä½¿ç”¨æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆå¦‚ Ridge æˆ– Lasso å›å½’ï¼‰  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02738f",
   "metadata": {},
   "source": [
    "# 5. Data Understanding & Preparation\n",
    "æœºå™¨å­¦ä¹ ç±»å‹æµç¨‹å›¾ï¼ˆMachine Learning Flowï¼‰ï¼š<br>\n",
    "**Questionï¼šIs labeled data available or can a target value be generated? æ˜¯å¦æœ‰æ ‡æ³¨æ•°æ®æˆ–å¯ä»¥ç”Ÿæˆç›®æ ‡å€¼ï¼Ÿ**  <br><br>\n",
    "\n",
    "Yes â†’ **ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰**\n",
    "- **å›å½’ï¼ˆRegressionï¼‰**: Used to predict continuous values (e.g., housing price, temperature)ç”¨äºé¢„æµ‹è¿ç»­æ•°å€¼ï¼ˆå¦‚æˆ¿ä»·ã€æ¸©åº¦ï¼‰\n",
    "  - Typesï¼šLinear Regression çº¿æ€§å›å½’ / Non-Linear Regression éçº¿æ€§å›å½’  \n",
    "- **åˆ†ç±»ï¼ˆClassificationï¼‰**: Used to predict categories (e.g., spam detection) ç”¨äºé¢„æµ‹ç±»åˆ«ï¼ˆå¦‚åƒåœ¾é‚®ä»¶è¯†åˆ«ï¼‰  \n",
    "  - Typesï¼šLinear Classification çº¿æ€§åˆ†ç±» / Non-Linear Classification éçº¿æ€§åˆ†ç±»  \n",
    "\n",
    "<br>\n",
    "\n",
    "No â†’ **æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰**\n",
    "- **èšç±»ï¼ˆClusteringï¼‰**ï¼šGrouping data without target values (e.g., customer segmentation) å°†æ•°æ®åˆ†ç»„ï¼Œæ— éœ€ç›®æ ‡å€¼ï¼ˆå¦‚å®¢æˆ·åˆ†ç¾¤ï¼‰  \n",
    "\n",
    "## 5.1 å»ºæ¨¡å‡è®¾ä¸ç‰¹å¾é€‰æ‹©ï¼ˆModeling Assumptions & Feature Selectionï¼‰\n",
    "å»ºæ¨¡å‡è®¾ï¼ˆModeling Assumptionsï¼‰\n",
    "- **ä»£è¡¨æ€§ï¼ˆRepresentativeï¼‰**ï¼šSamples should represent the overall population æ ·æœ¬åº”èƒ½ä»£è¡¨æ€»ä½“  \n",
    "- **ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆIID: Independent and Identically Distributedï¼‰**ï¼šEach row should be independent and from the same distribution æ¯è¡Œæ•°æ®åº”ç‹¬ç«‹ï¼Œä¸”æ¥è‡ªç›¸åŒåˆ†å¸ƒ\n",
    "\n",
    "<br>\n",
    "\n",
    "è¡Œå±‚é¢ï¼ˆRow-Levelï¼‰\n",
    "- **æµ‹é‡æ°´å¹³ï¼ˆMeasurement Levelï¼‰**ï¼šTypes of variables (e.g., numerical, categorical) å˜é‡ç±»å‹ï¼ˆæ•°å€¼å‹ã€ç±»åˆ«å‹ç­‰ï¼‰  \n",
    "- **åˆ†å¸ƒï¼ˆDistributionï¼‰**ï¼šVariable distribution (e.g., skewness, outliers) å˜é‡çš„åˆ†å¸ƒå½¢æ€ï¼ˆæ˜¯å¦åæ€ã€æ˜¯å¦æœ‰å¼‚å¸¸å€¼ï¼‰  \n",
    "\n",
    "<br>\n",
    "\n",
    "åˆ—å±‚é¢ï¼ˆColumn-Levelï¼‰\n",
    "- **å˜é‡å…³ç³»ï¼ˆRelationshipsï¼‰**ï¼šCorrelations between features and with target ç‰¹å¾ä¹‹é—´ã€ç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´çš„ç›¸å…³æ€§  \n",
    "\n",
    "\n",
    "## 5.2 æ•°æ®ç±»å‹è¯†åˆ«ï¼ˆData Type Recognitionï¼‰\n",
    "- **ç‹¬ç«‹æ•°æ®ï¼ˆIndependent Dataï¼‰**ï¼šObservations are unrelated (e.g., random sampling) è§‚æµ‹å€¼ä¹‹é—´æ— å…³è”ï¼ˆå¦‚éšæœºæŠ½æ ·ï¼‰  \n",
    "- **è‡ªç›¸å…³ï¼ˆAutocorrelationï¼‰**ï¼šNearby data in time or space are more similar (e.g., temperature) æ—¶é—´æˆ–ç©ºé—´ä¸Šç›¸è¿‘çš„æ•°æ®æ›´ç›¸ä¼¼ï¼ˆå¦‚æ¸©åº¦ï¼‰  \n",
    "- **ç±»å†…ç›¸å…³ï¼ˆIntraclass Correlationï¼‰**ï¼šCorrelation within groups (e.g., experimental groups) ç»„å†…æ•°æ®ç›¸å…³ï¼ˆå¦‚å®éªŒç»„ï¼‰  \n",
    "\n",
    "<br>\n",
    "åˆ¤æ–­æ–¹æ³•ï¼šCombine dataset purpose + visualization (e.g., scatterplot, autocorrelation plot) ç»“åˆæ•°æ®é›†ç›®çš„ + å¯è§†åŒ–ï¼ˆå¦‚æ•£ç‚¹å›¾ã€è‡ªç›¸å…³å›¾ï¼‰\n",
    "\n",
    "\n",
    "## 5.3 åˆ†ç®±ï¼ˆBinningï¼‰\n",
    "- **å®šä¹‰ï¼ˆDefinitionï¼‰**ï¼šConvert continuous variables into categories å°†è¿ç»­å˜é‡ç¦»æ•£åŒ–ä¸ºç±»åˆ«   \n",
    "- **ç¤ºä¾‹ï¼ˆExampleï¼‰**ï¼šWind speed is divided into é£é€Ÿåˆ†ä¸ºï¼š  \n",
    "  - \"low\" < 10 m/s  \n",
    "  - \"medium\" 10â€“15 m/s  \n",
    "  - \"high\" > 15 m/s\n",
    "- **ç”¨é€”ï¼ˆPurposeï¼‰**ï¼šSimplify model input for classification models ç®€åŒ–æ¨¡å‹è¾“å…¥ï¼Œé€‚åº”åˆ†ç±»æ¨¡å‹  \n",
    "\n",
    "## 5.4 æ»åç‰¹å¾å·¥ç¨‹ï¼ˆLag Feature Engineeringï¼‰\n",
    "- **å®šä¹‰ï¼ˆDefinitionï¼‰**ï¼šUse past observations as new features å°†è¿‡å»çš„è§‚æµ‹å€¼ä½œä¸ºæ–°ç‰¹å¾  \n",
    "  - Lag 1 = æ˜¨å¤©çš„å€¼ â†’ **Yesterday's value**  \n",
    "  - Lag 7 = ä¸Šå‘¨åŒä¸€å¤©çš„å€¼ â†’ **Same day last week**\n",
    "-  **é€‚ç”¨åœºæ™¯ï¼ˆWhen to Useï¼‰**ï¼šTime series modeling (e.g., predicting todayâ€™s temperature) æ—¶é—´åºåˆ—å»ºæ¨¡ï¼ˆå¦‚é¢„æµ‹ä»Šæ—¥æ¸©åº¦ï¼‰  \n",
    "\n",
    "## 5.5 è‡ªç›¸å…³ä¸è¶‹åŠ¿å¯è§†åŒ–ï¼ˆAutocorrelation & Trend Visualizationï¼‰\n",
    "\n",
    "### 5.5.1 è‡ªç›¸å…³ï¼ˆAutocorrelationï¼‰\n",
    "- Measures similarity between time series and its lagged version è¡¡é‡æ—¶é—´åºåˆ—ä¸å…¶æ»åç‰ˆæœ¬çš„ç›¸ä¼¼æ€§  \n",
    "- Bars outside confidence interval â†’ Significant autocorrelation exists æ¡å½¢å›¾è½åœ¨ç½®ä¿¡åŒºé—´å¤– â†’ å­˜åœ¨æ˜¾è‘—è‡ªç›¸å…³  \n",
    "  \n",
    "### 5.5.2 è¶‹åŠ¿å¯è§†åŒ–ï¼ˆTrend Visualizationï¼‰\n",
    "- **Histogram ç›´æ–¹å›¾**: Shows frequency of numerical variables å±•ç¤ºæ•°å€¼å‹å˜é‡çš„é¢‘ç‡åˆ†å¸ƒ\n",
    "- **Bar plot æŸ±çŠ¶å›¾**: Compares grouped categorical values æ¯”è¾ƒåˆ†ç±»å˜é‡ç»„çš„æ•°å€¼\n",
    "- **Box plot ç®±çº¿å›¾**: Visualizes distribution and outliers å¯è§†åŒ–åˆ†å¸ƒå½¢æ€åŠå¼‚å¸¸å€¼\n",
    "- **Scatterplot æ•£ç‚¹å›¾**: Shows relationship between two variables å±•ç¤ºä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³ç³»\n",
    "- **Line plot æŠ˜çº¿å›¾**: Suitable for time series é€‚ç”¨äºæ—¶é—´åºåˆ—åˆ†æ\n",
    "- **Heatmap çƒ­åŠ›å›¾**: Displays correlation or matrix values å±•ç¤ºç›¸å…³æ€§æˆ–çŸ©é˜µæ•°å€¼\n",
    "\n",
    "## 5.6 åˆ†ç±»å˜é‡ç¼–ç ï¼ˆCategorical Encodingï¼‰\n",
    "### 5.6.1 ç¼–ç æ–¹æ³•ï¼ˆEncoding Methodsï¼‰\n",
    "| æ–¹æ³•ï¼ˆMethodï¼‰       | è¯´æ˜ï¼ˆExplanationï¼‰     | é€‚ç”¨æ¨¡å‹ï¼ˆSuitable Modelsï¼‰ |\n",
    "|----------------------|--------------------------|------------------------------|\n",
    "| One-hot encoding     | æ¯ä¸ªç±»åˆ«ä¸€åˆ—             | éçº¿æ€§æ¨¡å‹ï¼ˆNon-linearï¼‰    |\n",
    "| Dummy encoding       | n ç±»åˆ« â†’ n-1 åˆ—          | çº¿æ€§æ¨¡å‹ï¼Œé¿å…å…±çº¿æ€§        |\n",
    "\n",
    "**Python Tools**ï¼š\n",
    "```python\n",
    "pd.get_dummies(drop_first=True)  # Dummy ç¼–ç \n",
    "sklearn.preprocessing.OneHotEncoder()  # One-hot ç¼–ç \n",
    "``` \n",
    "\n",
    "## 5.7 æ•°æ®åˆå¹¶ï¼ˆMergingï¼‰\n",
    "\n",
    "| åˆå¹¶æ–¹å¼ï¼ˆJoin Typeï¼‰ | ä¸­æ–‡è§£é‡Šï¼ˆExplanationï¼‰       | ç‰¹ç‚¹ï¼ˆFeatureï¼‰                     |\n",
    "|------------------------|-------------------------------|--------------------------------------|\n",
    "| Inner Join             | å†…è¿æ¥                        | ä»…ä¿ç•™ä¸¤è¡¨å…±æœ‰é”®ï¼ˆonly common keysï¼‰ |\n",
    "| Left Join              | å·¦è¿æ¥                        | ä¿ç•™å·¦è¡¨å…¨éƒ¨ + åŒ¹é…å³è¡¨              |\n",
    "| Right Join             | å³è¿æ¥                        | ä¿ç•™å³è¡¨å…¨éƒ¨ + åŒ¹é…å·¦è¡¨              |\n",
    "| Full Join              | å…¨è¿æ¥                        | ä¿ç•™æ‰€æœ‰è¡Œï¼Œç¼ºå¤±å¡« NaN               |\n",
    "\n",
    "## 5.8 ç¼ºå¤±å€¼å¤„ç†ï¼ˆMissing Valuesï¼‰\n",
    "\n",
    "### 5.8.1 ç¼ºå¤±ç±»å‹ï¼ˆTypes of Missingnessï¼‰\n",
    "| ç±»å‹ï¼ˆTypeï¼‰ | ä¸­æ–‡è§£é‡Šï¼ˆExplanationï¼‰          | ç¤ºä¾‹ï¼ˆExampleï¼‰                    |\n",
    "|--------------|----------------------------------|------------------------------------|\n",
    "| MCAR         | å®Œå…¨éšæœºç¼ºå¤±ï¼ˆMissing Completely at Randomï¼‰ | ç¡¬ä»¶æ•…éšœå¯¼è‡´æ•°æ®ä¸¢å¤±              |\n",
    "| MAR          | éšæœºç¼ºå¤±ï¼Œä¸å…¶ä»–å˜é‡ç›¸å…³ï¼ˆMissing At Randomï¼‰         | å¹´è½»äººä¸æ„¿æŠ¥å‘Šå±å¹•æ—¶é—´            |\n",
    "| MNAR         | ééšæœºç¼ºå¤±ï¼Œä¸ç¼ºå¤±å€¼æœ¬èº«ç›¸å…³ï¼ˆMissing Not At Randomï¼‰   | é‡åº¦å¸æ¯’è€…éšç’ä½¿ç”¨é¢‘ç‡            |\n",
    "\n",
    "### 5.8.2 ä¸ºä»€ä¹ˆé‡è¦ï¼ˆWhy It Mattersï¼‰\n",
    "- ä¼šå¼•å…¥åå·® â†’ May introduce bias  \n",
    "- é™ä½ç»Ÿè®¡æ•ˆèƒ½ â†’ Reduces statistical power  \n",
    "- å½±å“æ¨¡å‹è®­ç»ƒ â†’ Affects model training\n",
    "\n",
    "### 5.8.3 ç¼ºå¤±å€¼å¡«è¡¥æ–¹æ³•ï¼ˆImputation Methodsï¼‰\n",
    "| æ–¹æ³•ï¼ˆMethodï¼‰        | é€‚ç”¨æƒ…å†µï¼ˆWhen to Useï¼‰ | ä¼˜ç‚¹ï¼ˆAdvantagesï¼‰ | ç¼ºç‚¹ï¼ˆDisadvantagesï¼‰           |\n",
    "|------------------------|--------------------------|--------------------|----------------------------------|\n",
    "| åˆ é™¤ï¼ˆdropnaï¼‰         | MCAR                     | ç®€å•               | ä¸¢å¤±ä¿¡æ¯ï¼ˆInformation lossï¼‰    |\n",
    "| å‡å€¼/ä¸­ä½æ•°å¡«å……       | MCAR/MAR                 | å¿«é€Ÿ               | æ‰­æ›²åˆ†å¸ƒï¼ˆDistorts distributionï¼‰|\n",
    "| å‰å‘/åå‘å¡«å……         | æ—¶é—´åºåˆ—                 | ä¿æŒè¶‹åŠ¿           | ä¸é€‚ç”¨äºçªå˜                     |\n",
    "| çƒ­/å†·å¡ç‰‡å¡«å……         | æœ‰ç›¸ä¼¼å€¼å¯å€Ÿç”¨           | ä¿æŒåˆ†å¸ƒ           | å¯èƒ½å¼•å…¥åå·®                    |\n",
    "| æ’å€¼ï¼ˆinterpolationï¼‰ | æœ‰è¶‹åŠ¿å¯æ¨æ–­             | å¤šç§æ–¹æ³•           | ä¸é€‚ç”¨äºè·³è·ƒæ•°æ®                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25384ef6",
   "metadata": {},
   "source": [
    "# 6. å›å½’æ¨¡å‹ Regression Models\n",
    "\n",
    "CRISP-DM æ˜¯ä¸€ä¸ªæ•°æ®ç§‘å­¦é¡¹ç›®çš„æ¡†æ¶ï¼ŒåŒ…å«å…­ä¸ªé˜¶æ®µï¼šä¸šåŠ¡ç†è§£ã€æ•°æ®ç†è§£ã€æ•°æ®å‡†å¤‡ã€å»ºæ¨¡ã€è¯„ä¼°å’Œéƒ¨ç½²ã€‚ (CRISP-DM is a framework for data science projects, including six phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment.)\n",
    "- æ•°æ®ç†è§£ï¼šæ¢ç´¢å’Œæ€»ç»“æ•°æ®ï¼ˆä¾‹å¦‚æ£€æŸ¥æ¨¡å¼ã€å¼‚å¸¸å€¼æˆ–ç›¸å…³æ€§ï¼‰ï¼Œä¸ºå»ºæ¨¡åšå‡†å¤‡ã€‚ (Data Understanding: Explore and summarize data (e.g., check patterns, outliers, or correlations) to prepare for modeling.) æ•°æ®ç†è§£é€šè¿‡æ­ç¤ºæ¨¡å¼ï¼ˆä¾‹å¦‚çº¿æ€§æˆ–éçº¿æ€§å…³ç³»ï¼‰å¸®åŠ©é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚ (Data understanding helps select appropriate models by revealing patterns, such as linear or non-linear relationships.)\n",
    "- å»ºæ¨¡ï¼šåœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ éœ€è¦ï¼š (Modeling: In this phase, you need:)\n",
    "  - æ‹†åˆ†æ•°æ®ï¼šå°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå»ºç«‹æµ‹è¯•è®¾è®¡ã€‚ (Split data: Divide data into training and test sets to establish a test design.) æ‹†åˆ†æ•°æ®ç¡®ä¿æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šæµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ã€‚ (Splitting data ensures the model is tested on unseen data, simulating real-world scenarios.)\n",
    "  - æ„å»ºå’Œè®­ç»ƒæ¨¡å‹ï¼šç”¨è®­ç»ƒé›†è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ã€‚ (Build and train models: Train the model with the training set and make predictions on the test set.)\n",
    "\n",
    "ç›‘ç£å­¦ä¹ åŸºäºè¾“å…¥ï¼ˆç‰¹å¾ï¼‰å’Œè¾“å‡ºï¼ˆç›®æ ‡ï¼‰æ•°æ®å¼€å‘é¢„æµ‹æ¨¡å‹ï¼Œåˆ†ä¸ºåˆ†ç±»å’Œå›å½’ä¸¤å¤§ä»»åŠ¡ã€‚ (Supervised learning develops predictive models based on input (features) and output (target) data, divided into two main tasks: classification and regression.)\n",
    "- ç›‘ç£å­¦ä¹ ï¼šæ¨¡å‹ä»å·²çŸ¥æ­£ç¡®ç­”æ¡ˆï¼ˆç›®æ ‡ï¼‰çš„æ•°æ®ä¸­å­¦ä¹ ã€‚ (Supervised learning: The model learns from data with known correct answers (target).) ä¾‹å¦‚ï¼Œæœ‰æˆ¿å±‹é¢ç§¯ï¼ˆè¾“å…¥ï¼‰å’Œä»·æ ¼ï¼ˆè¾“å‡ºï¼‰çš„æ•°æ®ï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•æ ¹æ®é¢ç§¯é¢„æµ‹ä»·æ ¼ã€‚ (For example, with data on house size (input) and price (output), the model learns to predict price based on size.)\n",
    "- åˆ†ç±»ï¼šé¢„æµ‹ç±»åˆ«ï¼ˆä¾‹å¦‚â€œä¼šä¸‹é›¨å—ï¼Ÿâ€â†’ æ˜¯/å¦ï¼‰ã€‚ (Classification: Predict categories (e.g., â€œWill it rain?â€ â†’ Yes/No).)\n",
    "- å›å½’ï¼šé¢„æµ‹æ•°å€¼ï¼ˆä¾‹å¦‚â€œæˆ¿ä»·æ˜¯å¤šå°‘ï¼Ÿâ€â†’ 30ä¸‡ï¼‰ã€‚ (Regression: Predict numerical values (e.g., â€œWhat is the house price?â€ â†’ 300,000).) å›å½’ç”¨äºé¢„æµ‹æ•°å€¼ç»“æœï¼ˆå¦‚é”€å”®é¢æˆ–æ¸©åº¦ï¼‰ã€‚ (Regression is used to predict numerical outcomes, such as sales or temperature.)\n",
    "\n",
    "æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒæµç¨‹ï¼š (Core process of model training:)\n",
    "- Xï¼ˆç‰¹å¾/è‡ªå˜é‡ï¼‰ï¼šæ¨¡å‹è¾“å…¥ï¼Œæ˜¯æˆ‘ä»¬å·²çŸ¥çš„ã€‚ (X (features/independent variables): Model inputs, which are known to us.) ä¾‹å¦‚æ•°æ®æ¡†ä¸­çš„åˆ—ï¼Œå¦‚æˆ¿å±‹é¢ç§¯ã€å¹´é¾„ã€‚ (For example, columns in a dataset, such as house size or age.)\n",
    "- yï¼ˆç›®æ ‡å˜é‡/å› å˜é‡ï¼‰ï¼šæ¨¡å‹è¾“å‡ºï¼Œæ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„ï¼Œå¦‚æˆ¿ä»·ã€‚ (y (target variable/dependent variable): Model output, what we want to predict, such as house price.)\n",
    "\n",
    "è®­ç»ƒè¿‡ç¨‹ï¼š (Training process:)\n",
    "- åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚éšæœºæƒé‡ï¼‰ã€‚ (Initialize the model (e.g., with random weights).)\n",
    "- é¢„æµ‹yå€¼ã€‚ (Predict y values.)\n",
    "- è®¡ç®—è¯¯å·®ï¼ˆé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®ï¼‰ã€‚ (Calculate error (difference between predicted and actual values).)\n",
    "- æ›´æ–°æ¨¡å‹å‚æ•°ï¼ˆä¾‹å¦‚è°ƒæ•´æƒé‡ï¼‰ã€‚ (Update model parameters (e.g., adjust weights).)\n",
    "- é‡å¤ç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼ˆå¦‚è¯¯å·®è¶³å¤Ÿå°ï¼‰ã€‚ (Repeat until a stopping condition is met (e.g., error is small enough).)\n",
    "\n",
    "å»ºæ¨¡å‰çš„å‡è®¾ï¼š<br> (Assumptions before modeling:)<br>\n",
    "åœ¨è¿›è¡Œå»ºæ¨¡å‰ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥æ•°æ®æ˜¯å¦æ»¡è¶³ä¸€äº›æ¡ä»¶ï¼ˆå‡è®¾ï¼‰ï¼Œä¾‹å¦‚ï¼Œçº¿æ€§å›å½’å‡è®¾ç‰¹å¾å’Œç›®æ ‡å‘ˆç›´çº¿å…³ç³»ã€‚ (Before modeling, we need to check if the data meets certain conditions (assumptions), for example, linear regression assumes a linear relationship between features and target.) è¿™äº›æ¡ä»¶çš„å­˜åœ¨æ˜¯ä¸ºäº†ç¡®ä¿æˆ‘ä»¬å¾—åˆ°çš„ç»“æœæ˜¯å¯ä¿¡çš„ã€èƒ½è¢«æ­£ç¡®è§£é‡Šçš„ã€‚ (These conditions ensure the results are reliable and interpretable.)<br>\n",
    "å¦‚æœè¿™äº›å‡è®¾è¢«è¿èƒŒï¼Œæˆ‘ä»¬å°±å¯èƒ½éœ€è¦ï¼šæ”¹ç”¨å…¶ä»–æ¨¡å‹ï¼Œæˆ–è€…åœ¨æŠ¥å‘Šä¸­è¯´æ˜ç»“æœå¯èƒ½ä¼šå¤±çœŸã€‚ (If these assumptions are violated, we may need to switch to other models or explain in the report that results may be distorted.)\n",
    "\n",
    "## 6.1 çº¿æ€§å›å½’ Linear Regression\n",
    "çº¿æ€§å›å½’å»ºæ¨¡çš„ç›®çš„æ˜¯æ‰¾åˆ°ä¸æ‰€æœ‰æ•°æ®ç‚¹æœ€æ¥è¿‘çš„ç›´çº¿ï¼Œå°½é‡å‡å°‘è¯¯å·®ï¼ˆé¢„æµ‹å€¼ä¸å®é™…å€¼çš„å·®è·ï¼‰ã€‚ (The purpose of linear regression is to find the line closest to all data points, minimizing errors (difference between predicted and actual values).) çº¿æ€§å›å½’ç®€å•ä¸”æ˜“äºè§£é‡Šï¼Œæ˜¯æ•°å€¼é¢„æµ‹çš„è‰¯å¥½èµ·ç‚¹ã€‚ (Linear regression is simple and interpretable, a good starting point for numerical predictions.)<br>\n",
    "æ ‡å‡†çš„ä¸€å…ƒçº¿æ€§å›å½’å…¬å¼æ˜¯ï¼šy = Î²0 + Î²1 * X + Ïµ (The standard simple linear regression formula is: y = Î²0 + Î²1 * X + Ïµ)\n",
    "- ğ‘¦ï¼šé¢„æµ‹å€¼ã€‚ (Predicted value.)\n",
    "- Î²0ï¼šæˆªè·ï¼Œå³å½“X=0æ—¶çš„yå€¼ï¼ˆåŸºçº¿ï¼‰ã€‚ (Intercept, the value of y when X=0 (baseline).)\n",
    "- Î²1ï¼šæ–œç‡ï¼Œè¡¨ç¤ºXæ¯å¢åŠ 1ï¼Œyä¼šå¢åŠ å¤šå°‘ã€‚ (Slope, indicating how much y increases per unit of X.)\n",
    "- Xï¼šè‡ªå˜é‡ã€‚ (Independent variable.)\n",
    "- Ïµï¼šè¯¯å·®é¡¹ï¼ˆçœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®å¼‚ï¼‰ã€‚ (Error term (difference between actual and predicted values).)\n",
    "\n",
    "å¤šå…ƒçº¿æ€§å›å½’ä¸ç³»æ•°è§£é‡Šï¼šé¢å¯¹å¤šä¸ªç‰¹å¾æ—¶çš„æƒ…å†µï¼Œå¦‚æœæœ‰å¤šä¸ªè‡ªå˜é‡ï¼ˆç‰¹å¾ï¼‰ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªå›å½’å¹³é¢è€Œä¸æ˜¯å›å½’çº¿ã€‚ (Multiple linear regression and coefficient interpretation: In cases with multiple features, if there are multiple independent variables (features), we get a regression plane instead of a line.) ç³»æ•°è¡¨ç¤ºè¯¥ç‰¹å¾å¯¹ç›®æ ‡å˜é‡çš„å½±å“ç¨‹åº¦ã€‚ (Coefficients indicate the impact of each feature on the target variable.) å¦‚æœæˆ‘ä»¬å°†æ¯ä¸ªç‰¹å¾çš„ç³»æ•°ä¹˜ä»¥å®ƒçš„æ ‡å‡†å·®ï¼Œå¯ä»¥è¡¡é‡å®ƒå¯¹yçš„â€œå®é™…å½±å“åŠ›â€ã€‚ (If we multiply each featureâ€™s coefficient by its standard deviation, we can measure its â€œactual impactâ€ on y.)\n",
    "\n",
    "çº¿æ€§æ¨¡å‹çš„ç±»å‹ï¼ˆLasso ä¸ Ridgeï¼‰ï¼š (Types of linear models (Lasso and Ridge):)\n",
    "1. Lasso å›å½’ï¼ˆL1æ­£åˆ™åŒ–ï¼‰ï¼šä¼šå°†ä¸é‡è¦çš„ç‰¹å¾ç³»æ•°å‹ç¼©ä¸º0 â†’ å®ç°ç‰¹å¾é€‰æ‹©ï¼Œé€‚ç”¨äºå°‘é‡ç‰¹å¾æœ‰ç”¨çš„æƒ…å†µã€‚ (Lasso regression (L1 regularization): Compresses coefficients of unimportant features to 0 â†’ achieves feature selection, suitable when only a few features are useful.)\n",
    "2. Ridge å›å½’ï¼ˆL2æ­£åˆ™åŒ–ï¼‰ï¼šä¸ä¼šå°†ç³»æ•°å‹ä¸º0ï¼Œä½†ä¼šç¼©å°å®ƒä»¬ â†’ é™ä½è¿‡æ‹Ÿåˆé£é™©ï¼Œé€‚ç”¨äºæ‰€æœ‰ç‰¹å¾å¯èƒ½éƒ½æœ‰ç”¨çš„æƒ…å†µã€‚ (Ridge regression (L2 regularization): Does not compress coefficients to 0 but shrinks them â†’ reduces overfitting risk, suitable when all features may be useful.)\n",
    "\n",
    "### 6.1.1 çº¿æ€§å›å½’å‡è®¾ Linear Regression Assumptions\n",
    "çº¿æ€§å›å½’ä¾èµ–å››ä¸ªå…³é”®å‡è®¾ï¼š (Linear regression relies on four key assumptions:)\n",
    "\n",
    "| å‡è®¾ Assumption | å«ä¹‰ Meaning | æ£€æŸ¥æ–¹å¼ Check Method | ä¿®å¤æ–¹å¼ Fix Method | ä¸¾ä¾‹ Example |\n",
    "|----------------|-------------|---------------|---------------|---------------|\n",
    "| **çº¿æ€§å…³ç³» Linear relationship** | æ¯ä¸ªç‰¹å¾ä¸ç›®æ ‡çš„å…³ç³»å¿…é¡»æ˜¯ç›´çº¿ã€‚ (Each featureâ€™s relationship with the target must be linear.) | æ•£ç‚¹å›¾ / Pairplot (Scatter plot / Pairplot) | ä½¿ç”¨éçº¿æ€§æ¨¡å‹ (Use non-linear models) | å¦‚æœé”€å”®é¢éšå¹¿å‘Šæ”¯å‡ºç¨³å®šå¢åŠ ï¼Œæ˜¯çº¿æ€§å…³ç³»ï¼›å¦‚æœè¾¾åˆ°æŸç‚¹åè¶‹å¹³ï¼Œåˆ™æ˜¯éçº¿æ€§ã€‚ (If sales increase steadily with ad spend, itâ€™s linear; if it plateaus after a point, itâ€™s non-linear.) |\n",
    "| **è§‚å¯Ÿå€¼ç‹¬ç«‹ Independence of observations** | æ¯ä¸ªæ•°æ®ç‚¹ï¼ˆè¡Œï¼‰ä¸åº”å½±å“å…¶ä»–ç‚¹ã€‚ (Each data point (row) should not affect others.) æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä¾‹å¦‚æ¯æ—¥é”€å”®é¢ï¼‰å¸¸è¿åæ­¤å‡è®¾ï¼Œå› ä¸ºä»Šå¤©çš„æ•°æ®å¯èƒ½ä¾èµ–æ˜¨å¤©ã€‚ (Time series data (e.g., daily sales) often violates this because todayâ€™s data may depend on yesterdayâ€™s.) | Durbin-Watsonæ£€éªŒ (Durbin-Watson test) | åŠ å…¥æ»åå˜é‡ / æ”¹ç”¨æ—¶åºæ¨¡å‹ (Add lag variables / Use time series models) |  |\n",
    "| **æ— å¤šé‡å…±çº¿æ€§ No multicollinearity** | ç‰¹å¾ä¹‹é—´ä¸å¼ºç›¸å…³ï¼ˆä¾‹å¦‚æˆ¿å±‹é¢ç§¯å’Œæˆ¿é—´æ•°é«˜åº¦ç›¸å…³ï¼Œæä¾›ç±»ä¼¼ä¿¡æ¯ï¼‰ã€‚ (Features should not be strongly correlated (e.g., house size and number of rooms are highly correlated, providing similar information).) | çƒ­åŠ›å›¾ã€VIF > 5 (Heatmap, VIF > 5) | åˆ é™¤æˆ–åˆå¹¶å˜é‡ (Remove or combine variables) | é¢„æµ‹æ±½è½¦ä»·æ ¼æ—¶ï¼Œå¦‚æœå¼•æ“å¤§å°å’Œé©¬åŠ›é«˜åº¦ç›¸å…³ï¼Œé‚£å°±è¦ç§»é™¤ä¸€ä¸ªæˆ–åˆå¹¶ã€‚ (When predicting car prices, if engine size and horsepower are highly correlated, remove one or combine them.) |\n",
    "| **æ®‹å·®æ­£æ€åˆ†å¸ƒ/åŒæ–¹å·®æ€§ Normality of residuals/Homoscedasticity** | è¯¯å·®åº”å¹³å‡åˆ†å¸ƒä¸”æ–¹å·®ä¸€è‡´ã€‚ (Errors should be evenly distributed with constant variance.) æ®‹å·®æŒ‡é¢„æµ‹å€¼ä¸å®é™…å€¼çš„å·®ï¼›æ­£æ€åˆ†å¸ƒæŒ‡æ®‹å·®åº”å‘ˆé’Ÿå½¢ï¼ˆæ­£æ€ï¼‰åˆ†å¸ƒï¼›åŒæ–¹å·®æ€§æŒ‡æ®‹å·®çš„æ–¹å·®åœ¨æ‰€æœ‰é¢„æµ‹å€¼ä¸­åº”æ’å®šï¼ˆæ®‹å·®å›¾æ— æ¼æ–—å½¢ï¼‰ã€‚ (Residuals are the difference between predicted and actual values; normality means residuals should follow a bell-shaped (normal) distribution; homoscedasticity means residualsâ€™ variance should be constant across all predicted values (no funnel shape in residual plots).) | æ®‹å·®å›¾ã€Q-Qå›¾ (Residual plots, Q-Q plots) | å˜é‡è½¬æ¢ï¼ˆä¾‹å¦‚å¯¹yå–å¯¹æ•°ï¼‰æˆ–æ¢æ¨¡å‹ (Variable transformation (e.g., log-transform y) or change models) | æ¯”å¦‚ï¼Œå¦‚æœæ®‹å·®å›¾å‘ˆæ¼æ–—å½¢ï¼ˆé¢„æµ‹å€¼é«˜æ—¶è¯¯å·®æ›´å¤§ï¼‰ï¼Œè¯´æ˜æ–¹å·®ä¸ä¸€è‡´ï¼Œéœ€æ¢æ¨¡å‹ã€‚ (For example, if residual plots show a funnel shape (larger errors at higher predictions), it indicates non-constant variance, requiring a different model.) |\n",
    "\n",
    "## 6.2 éçº¿æ€§æ¨¡å‹ Non-linear Models\n",
    "å¦‚æœæ•°æ®å‘ˆéçº¿æ€§å…³ç³»ï¼ˆå½“Xå’Œyä¸å‘ˆç›´çº¿å…³ç³»ï¼Œä¾‹å¦‚é”€å”®é¢æœ€åˆå¿«é€Ÿå¢åŠ ä¹‹ååˆå‡ç¼“ï¼‰ï¼Œä½¿ç”¨ä»¥ä¸‹æ¨¡å‹ï¼š (If the data shows a non-linear relationship (when X and y are not linearly related, e.g., sales increase rapidly at first then slow down), use the following models:)\n",
    "- å†³ç­–æ ‘å›å½’ï¼šç®€å•ä½†å®¹æ˜“è¿‡æ‹Ÿåˆã€‚ (Decision tree regression: Simple but prone to overfitting.)\n",
    "- éšæœºæ£®æ—å›å½’ï¼šç”¨å¤šä¸ªæ ‘å¹³å‡é¢„æµ‹ï¼Œæ›´ç¨³å¥ã€‚ (Random forest regression: Averages predictions from multiple trees, more robust.) å¹³å‡é¢„æµ‹ï¼Œå‡å°‘è¿‡æ‹Ÿåˆã€‚ (Average predictions, reducing overfitting.)\n",
    "- æ¢¯åº¦æå‡å›å½’ï¼šé€æ£µæ ‘æ„å»ºï¼Œçº æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ï¼Œé€šå¸¸æ›´å‡†ç¡®ã€‚ (Gradient boosting regression: Builds trees sequentially, correcting errors of previous trees, usually more accurate.)\n",
    "\n",
    "## 6.3 æ—¶é—´åºåˆ—æ¨¡å‹ Time Series Models\n",
    "çº¿æ€§å›å½’ç­‰æ ‡å‡†æ¨¡å‹ä¸é€‚åˆæ—¶é—´åºåˆ—ï¼Œå› ä¸ºæ•°æ®ç‚¹æœ‰æ—¶é—´ä¾èµ–æ€§ï¼ˆè‡ªç›¸å…³ï¼‰ã€‚ (Standard models like linear regression are unsuitable for time series due to temporal dependencies (autocorrelation).) æ—¶é—´åºåˆ—ï¼ˆæ•°æ®æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼‰æ•°æ®ï¼ˆä¾‹å¦‚æ¯æ—¥è‚¡ä»·ã€å¤©æ°”ã€é”€å”®é¢ï¼‰éœ€è¦ç‰¹æ®Šæ¨¡å‹ï¼š (Time series (data ordered by time) data (e.g., daily stock prices, weather, sales) requires special models:)\n",
    "- ç§»åŠ¨å¹³å‡ï¼šå–å‰å‡ æœŸå€¼çš„å¹³å‡å€¼é¢„æµ‹ä¸‹ä¸€æœŸï¼ˆä¾‹å¦‚ç”¨å‰7å¤©é”€å”®é¢å¹³å‡å€¼é¢„æµ‹æ˜å¤©ï¼‰ï¼Œå¯ä»¥å¹³æ»‘å™ªå£°ã€‚ (Moving average: Takes the average of previous periods to predict the next (e.g., use the average of the last 7 daysâ€™ sales to predict tomorrow), smoothing noise.)\n",
    "- (S)ARIMA(X)ï¼šé«˜çº§æ¨¡å‹ï¼Œé¢„æµ‹è¶‹åŠ¿å’Œå­£èŠ‚æ€§ã€‚ (Advanced model for forecasting trends and seasonality.)\n",
    "\n",
    "## 6.4 å›å½’æŒ‡æ ‡ Regression Metrics\n",
    "\n",
    "| æŒ‡æ ‡ Metric | è¯´æ˜ Description | é€‚ç”¨åœºæ™¯ Applicable Scenarios |\n",
    "|--------------------------|---------------------|---------------------------|\n",
    "| **MAE å¹³å‡ç»å¯¹è¯¯å·® Mean Absolute Error** | é¢„æµ‹å€¼ä¸å®é™…å€¼çš„å¹³å‡ç»å¯¹å·®ï¼Œå•ä½ä¸ç›®æ ‡ç›¸åŒã€‚ (The average absolute difference between predicted and actual values, in the same units as the target.) æ‰€æœ‰é”™è¯¯å¹³ç­‰å¯¹å¾…ã€‚ (All errors are treated equally.) å¦‚æœæˆ¿ä»·é¢„æµ‹çš„MAE=1ä¸‡ï¼Œè¯´æ˜å¹³å‡è¯¯å·®ä¸º1ä¸‡å…ƒã€‚ (If MAE=10,000 for house prices, the average error is 10,000.) | æ—¥å¸¸ä½¿ç”¨ (Everyday use) |\n",
    "| **MSE / RMSE å‡æ–¹è¯¯å·® / æ ¹å‡æ–¹è¯¯å·® Mean Squared Error / Root Mean Squared Error** | å…ˆå¹³æ–¹è¯¯å·®å†å¹³å‡ç„¶åå¼€æ ¹ï¼Œæ”¾å¤§å¤§è¯¯å·®çš„å½±å“ï¼Œé€‚åˆå¤§è¯¯å·®ä»£ä»·é«˜çš„æƒ…å†µï¼ˆä¾‹å¦‚è¯ç‰©å‰‚é‡é¢„æµ‹ï¼‰ã€‚ (Squares errors, averages them, then takes the square root, emphasizing larger errors, suitable for cases where large errors are costly (e.g., drug dosage prediction).) | é«˜é£é™©é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ï¼‰ (High-risk areas (e.g., healthcare)) |\n",
    "| **RÂ² å†³å®šç³»æ•° Coefficient of Determination** | æ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹ï¼Œä»…é€‚ç”¨äº**çº¿æ€§æ¨¡å‹**ã€‚ (The proportion of variance explained by the model, only reliable for **linear models**.) | è¶Šæ¥è¿‘1è¶Šå¥½ï¼ˆæ³¨æ„ï¼šéçº¿æ€§æ¨¡å‹ä¸­RÂ²å¯èƒ½æ— æ„ä¹‰æˆ–ä¸ºè´Ÿï¼‰ (Closer to 1 is better (note: RÂ² may be meaningless or negative in non-linear models)) |\n",
    "\n",
    "## 6.5 æ•°æ®åˆ‡åˆ†ä¸éªŒè¯ Data Splitting and Validationï¼ˆTrain-Test Split & Cross-Validationï¼‰\n",
    "\n",
    "**è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ† Train-Test Split**ï¼š\n",
    "- è®­ç»ƒé›†ï¼šç”¨äºæ‹Ÿåˆæ¨¡å‹ï¼ˆä¾‹å¦‚80%ï¼‰ã€‚ (Training set: Used to fit the model (e.g., 80%).)\n",
    "- æµ‹è¯•é›†ï¼šç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®ä¸Šçš„è¡¨ç°ï¼ˆä¾‹å¦‚20%ï¼‰ã€‚ (Test set: Used to evaluate the model on unseen data (e.g., 20%).)\n",
    "\n",
    "æ¯”å¦‚é¢„æµ‹è€ƒè¯•æˆç»©ï¼Œç”¨80%å­¦ç”Ÿæ•°æ®è®­ç»ƒï¼Œ20%æµ‹è¯•ï¼Œæ£€æŸ¥æ¨¡å‹æ˜¯å¦èƒ½é¢„æµ‹æ–°å­¦ç”Ÿçš„æˆç»©ã€‚ (For example, predict exam scores using 80% of student data for training and 20% for testing to check if the model predicts new studentsâ€™ scores.)\n",
    "\n",
    "**äº¤å‰éªŒè¯ï¼ˆK-Foldï¼‰ Cross-Validation (K-Fold)**ï¼š\n",
    "KæŠ˜äº¤å‰éªŒè¯å°†æ•°æ®åˆ†æˆKä»½ï¼Œè®­ç»ƒç”¨K-1ä»½ï¼Œæµ‹è¯•ç”¨1ä»½ï¼Œé‡å¤Kæ¬¡ã€‚ (K-fold cross-validation divides data into K parts, trains on K-1 parts, tests on 1 part, and repeats K times.) å¦‚æœK=5ï¼Œå°†æ•°æ®åˆ†æˆ5ä»½ï¼Œæ¯æ¬¡ç”¨4ä»½è®­ç»ƒï¼Œ1ä»½æµ‹è¯•ï¼Œé‡å¤5æ¬¡ï¼Œæ¯æ¬¡ç”¨ä¸åŒä»½æµ‹è¯•ï¼Œå¹³å‡ç»“æœä»¥è·å¾—å¯é æ€§èƒ½ä¼°è®¡ã€‚ (If K=5, divide data into 5 parts, each time train on 4 parts, test on 1, repeat 5 times with different test parts, and average results for a reliable performance estimate.) ç‰¹åˆ«é€‚åˆå°æ•°æ®é›†ï¼Œæœ€å¤§åŒ–æ•°æ®ä½¿ç”¨ï¼ŒåŒæ—¶é¿å…è¿‡æ‹Ÿåˆã€‚ (Especially suitable for small datasets, maximizing data use while avoiding overfitting.) æ¯”å¦‚100ä¸ªæˆ¿ä»·æ•°æ®ï¼Œ5æŠ˜äº¤å‰éªŒè¯åˆ†æˆ5ç»„ï¼Œæ¯ç»„20ä¸ªã€‚ (For example, 100 house prices, 5-fold CV divides into 5 groups of 20.) æ¯æ¬¡ç”¨80ä¸ªè®­ç»ƒï¼Œ20ä¸ªæµ‹è¯•ï¼Œé‡å¤5æ¬¡ï¼Œå¹³å‡è¯¯å·®ä»¥è¯„ä¼°æ¨¡å‹ã€‚ (Each time train on 80, test on 20, repeat 5 times, and average errors to evaluate the model.)\n",
    "\n",
    "**æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ Time Series Cross-Validation**ï¼š\n",
    "å¯¹æ—¶é—´åºåˆ—æˆ–éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-i.i.d.ï¼‰æ•°æ®ï¼Œä½¿ç”¨TimeSeriesSplitæŒ‰æ—¶é—´é¡ºåºæ‹†åˆ†æ•°æ®ï¼Œé¿å…éšæœºæ‹†åˆ†ã€‚ (For time series or non-independent and identically distributed (non-i.i.d.) data, use TimeSeriesSplit to split data chronologically, avoiding random splitting.) æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä¾‹å¦‚è‚¡ä»·ï¼‰æœ‰æ—¶é—´ä¾èµ–æ€§ï¼Œéšæœºæ‹†åˆ†ä¼šå¯¼è‡´æ•°æ®æ³„éœ²ï¼ˆç”¨æœªæ¥æ•°æ®é¢„æµ‹è¿‡å»ï¼‰ã€‚ (Time series data (e.g., stock prices) has temporal dependencies; random splitting causes data leakage (using future data to predict the past).) TimeSeriesSplitæŒ‰å›ºå®šæ—¶é—´é—´éš”æ‹†åˆ†ï¼ˆä¾‹å¦‚ç”¨ç¬¬1â€“80å¤©è®­ç»ƒï¼Œç¬¬81â€“100å¤©æµ‹è¯•ï¼Œç„¶åç”¨ç¬¬1â€“100å¤©è®­ç»ƒï¼Œç¬¬101â€“120å¤©æµ‹è¯•ï¼‰ï¼Œç¡®ä¿æ—¶é—´ä¾èµ–æ•°æ®çš„çœŸå®é¢„æµ‹ï¼Œé˜²æ­¢è¯¯å¯¼æ€§å¥½ç»“æœã€‚ (TimeSeriesSplit splits by fixed time intervals (e.g., train on days 1â€“80, test on days 81â€“100, then train on days 1â€“100, test on days 101â€“120), ensuring realistic predictions for time-dependent data, preventing misleadingly good results.)\n",
    "\n",
    "## 6.6 å¸¸è§å›å½’æ¨¡å‹å¯¹æ¯”ï¼ˆæ€»ç»“ï¼‰ Comparison of Common Regression Models (Summary)\n",
    "\n",
    "| æ¨¡å‹ç±»å‹ Model Type | ä¼˜åŠ¿ Advantages | é€‚ç”¨åœºæ™¯ Applicable Scenarios |\n",
    "|----------------|-----------|------------|\n",
    "| **çº¿æ€§å›å½’ Linear Regression** | ç®€å•æ˜“è§£é‡Š (Simple and interpretable) | çº¿æ€§å…³ç³»æ˜æ˜¾ï¼Œæ•°æ®è´¨é‡é«˜ (Clear linear relationships, high-quality data) |\n",
    "| **Ridge / Lasso** | å¤„ç†ç‰¹å¾ç›¸å…³æ€§ï¼ŒRidgeç¼©å°ç›¸å…³ç‰¹å¾å½±å“ï¼ŒLassoå°†ä¸é‡è¦ç‰¹å¾ç½®é›¶ã€‚ (Handle feature correlation, Ridge shrinks correlated feature impacts, Lasso sets unimportant features to zero.) æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ (Regularization prevents overfitting) | å¤šç‰¹å¾ã€å¯èƒ½ç›¸å…³ (Multiple features, possible correlations) |\n",
    "| **éšæœºæ£®æ— Random Forest** | é€‚åˆéçº¿æ€§æ•°æ®ï¼Œç»“åˆå¤šä¸ªå†³ç­–æ ‘ã€‚ (Suitable for non-linear data, combines multiple decision trees.) æŠ—è¿‡æ‹Ÿåˆ (Robust against overfitting) | ç‰¹å¾ä¹‹é—´å¤æ‚å…³ç³» (Complex relationships between features) |\n",
    "| **SVMå›å½’ SVM Regression** | é€‚åˆé«˜ç»´ã€éçº¿æ€§æ•°æ® (Suitable for high-dimensional, non-linear data) | ç‰¹å¾ç»´åº¦è¿œå¤§äºæ ·æœ¬æ•° (Feature dimensions far exceed sample size) |\n",
    "| **Logisticå›å½’ Logistic Regression** | å¸¸ç”¨äºåˆ†ç±»ï¼Œä½†ä¹Ÿå¯ç”¨äºå›å½’ï¼Œé¢„æµ‹äº‹ä»¶æ¦‚ç‡ (Commonly used for classification but can be used for regression, predicting event probabilities) | äºŒåˆ†ç±»/æ¦‚ç‡è¾“å‡ºé—®é¢˜ (Binary classification/probability output problems) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb21223",
   "metadata": {},
   "source": [
    "# 7. Classificationï¼ˆåˆ†ç±»ï¼‰\n",
    "\n",
    "åˆ†ç±»æ˜¯ç›‘ç£å­¦ä¹ ä¸­çš„ä¸€ç§ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯æ ¹æ®è¾“å…¥æ•°æ®é¢„æµ‹ç¦»æ•£çš„ç±»åˆ«æ ‡ç­¾ã€‚  \n",
    "Classification is a type of supervised learning where the goal is to predict discrete class labels based on input data.\n",
    "\n",
    "åˆ†ç±»æ¨¡å‹å¹¿æ³›åº”ç”¨äºå¤šä¸ªå®é™…åœºæ™¯ï¼Œä¾‹å¦‚ç–¾ç—…è¯Šæ–­ã€åƒåœ¾é‚®ä»¶è¿‡æ»¤ã€ä¿¡ç”¨è¯„åˆ†ç­‰ã€‚  \n",
    "Classification models are widely used in real-world scenarios such as disease diagnosis, spam detection, and credit scoring.\n",
    "\n",
    "## 7.1 åˆ†ç±»çš„ç±»å‹ï¼ˆTypes of Classificationï¼‰\n",
    "\n",
    "- **äºŒå…ƒåˆ†ç±»ï¼ˆBinary Classificationï¼‰**ï¼šåªæœ‰ä¸¤ä¸ªç±»åˆ«ï¼Œä¾‹å¦‚â€œæ˜¯/å¦â€ã€â€œåƒåœ¾é‚®ä»¶/éåƒåœ¾é‚®ä»¶â€ã€‚  \n",
    "  Binary Classification: Two possible classes, e.g., \"yes/no\", \"spam/not spam\".\n",
    "- **å¤šç±»åˆ†ç±»ï¼ˆMulticlass Classificationï¼‰**ï¼šä¸‰ä¸ªæˆ–ä»¥ä¸Šçš„äº’æ–¥ç±»åˆ«ï¼Œä¾‹å¦‚â€œç‹—â€ã€â€œçŒ«â€ã€â€œé©¬â€ã€â€œè€é¼ â€ã€‚  \n",
    "  Multiclass Classification: Three or more mutually exclusive classes, e.g., \"dog\", \"cat\", \"horse\", \"mouse\".\n",
    "- **å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆMultilabel Classificationï¼‰**ï¼šæ¯ä¸ªæ ·æœ¬å¯èƒ½å¯¹åº”å¤šä¸ªæ ‡ç­¾ï¼Œä¾‹å¦‚ä¸€å¼ å›¾åƒä¸­æ—¢æœ‰ç‹—åˆæœ‰çŒ«ã€‚  \n",
    "  Multilabel Classification: Each sample may belong to multiple classes simultaneously, e.g., \"dog & cat\".\n",
    "\n",
    "---\n",
    "\n",
    "## 7.2 Classification Modelsï¼ˆåˆ†ç±»æ¨¡å‹ï¼‰\n",
    "\n",
    "åˆ†ç±»ä»»åŠ¡å¯ä½¿ç”¨å¤šç§çº¿æ€§æˆ–éçº¿æ€§æ¨¡å‹ã€‚  \n",
    "A variety of linear and non-linear models can be used for classification tasks.\n",
    "\n",
    "### 7.2.1 Logistic Regressionï¼ˆé€»è¾‘å›å½’ï¼‰\n",
    "\n",
    "é€»è¾‘å›å½’ç”¨äºä¼°ç®—æ¦‚ç‡å¹¶è¿›è¡ŒäºŒåˆ†ç±»ã€‚  \n",
    "Logistic regression estimates probabilities and performs binary classification.\n",
    "\n",
    "- ä½¿ç”¨ sigmoidï¼ˆé€»è¾‘ï¼‰å‡½æ•°å°†è¾“å…¥æ˜ å°„ä¸º0åˆ°1ä¹‹é—´çš„æ¦‚ç‡å€¼  \n",
    "  Uses a sigmoid function to map input to probabilities between 0 and 1\n",
    "- ä½¿ç”¨é˜ˆå€¼æ¥ç¡®å®šåˆ†ç±»ç»“æœ  \n",
    "  Applies a threshold to decide the classification\n",
    "- å‡è®¾ç‰¹å¾ä¸ç»“æœä¹‹é—´çº¿æ€§å…³ç³»  \n",
    "  Assumes a linear relationship between features and outcomes\n",
    "- å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ  \n",
    "  Sensitive to outliers\n",
    "\n",
    "### 7.2.2 Support Vector Machinesï¼ˆæ”¯æŒå‘é‡æœºï¼‰\n",
    "\n",
    "æ”¯æŒå‘é‡æœºé€šè¿‡å¯»æ‰¾æœ€å¤§é—´éš”çš„è¶…å¹³é¢æ¥åˆ’åˆ†ç±»åˆ«ã€‚  \n",
    "Support Vector Machines (SVM) find a hyperplane that maximizes the margin between classes.\n",
    "\n",
    "- é«˜ç»´ç‰¹å¾è¡¨ç°è‰¯å¥½  \n",
    "  Works well in high-dimensional spaces\n",
    "- å…³é”®è¶…å‚æ•°åŒ…æ‹¬ï¼š\n",
    "  - **C** æ§åˆ¶é—´éš”å¤§å°ï¼ˆæ­£åˆ™åŒ–ï¼‰  \n",
    "    C controls the margin size (regularization)\n",
    "  - **gamma** æ§åˆ¶å•ä¸ªæ ·æœ¬çš„å½±å“åŠ›  \n",
    "    gamma controls the influence of individual samples\n",
    "- å¯ä½¿ç”¨éçº¿æ€§æ ¸å‡½æ•°å¤„ç†éçº¿æ€§æ•°æ®  \n",
    "  Non-linear kernels can handle non-linear data\n",
    "- ä½¿ç”¨åº“ï¼š`sklearn.svm.SVC`  \n",
    "  Library used: `sklearn.svm.SVC`\n",
    "\n",
    "### 7.2.3 Decision Treeï¼ˆå†³ç­–æ ‘ï¼‰\n",
    "\n",
    "å†³ç­–æ ‘é€šè¿‡åŸºäºç‰¹å¾çš„å€¼åˆ’åˆ†æ•°æ®ï¼Œæ„å»ºå…·æœ‰å†³ç­–è·¯å¾„çš„æ ‘ç»“æ„ã€‚  \n",
    "Decision trees split data based on feature values to form a tree with decision paths.\n",
    "\n",
    "- æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç‰¹å¾ï¼Œæ¯æ¡åˆ†æ”¯æ˜¯å†³ç­–è§„åˆ™ï¼Œå¶èŠ‚ç‚¹æ˜¯åˆ†ç±»ç»“æœ  \n",
    "  Each node represents a feature, each branch a decision rule, and each leaf a result\n",
    "- ç›´è§‚ã€æ˜“è§£é‡Š  \n",
    "  Interpretable and intuitive\n",
    "- æ˜“è¿‡æ‹Ÿåˆï¼Œå¯é€šè¿‡é¢„å‰ªææˆ–åå‰ªææ§åˆ¶å¤æ‚åº¦  \n",
    "  Easily overfits; controlled via pre-pruning (e.g., max_depth) or post-pruning\n",
    "- ä½¿ç”¨åº“ï¼š`sklearn.tree.DecisionTreeClassifier`  \n",
    "  Library used: `sklearn.tree.DecisionTreeClassifier`\n",
    "\n",
    "### 7.2.4 Random Forestï¼ˆéšæœºæ£®æ—ï¼‰\n",
    "\n",
    "éšæœºæ£®æ—é€šè¿‡æ„å»ºå¤šä¸ªå†³ç­–æ ‘å¹¶æŠ•ç¥¨å†³å®šæœ€ç»ˆåˆ†ç±»ç»“æœã€‚  \n",
    "Random Forest builds multiple decision trees and combines their results through voting.\n",
    "\n",
    "- æ˜¯ä¸€ç§é›†æˆæ–¹æ³•ï¼ˆEnsemble Methodï¼‰  \n",
    "  An ensemble method\n",
    "- ä½¿ç”¨ Baggingï¼ˆå¯¹æ ·æœ¬éšæœºæŠ½æ ·ï¼‰ å’Œéšæœºç‰¹å¾é€‰æ‹©  \n",
    "  Uses bagging and random feature selection\n",
    "- æ›´é²æ£’ã€æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›å¼º  \n",
    "  More robust and less prone to overfitting\n",
    "- å¯è®¡ç®—ç‰¹å¾é‡è¦æ€§  \n",
    "  Can compute feature importance\n",
    "- ä½¿ç”¨åº“ï¼š`sklearn.ensemble.RandomForestClassifier`  \n",
    "  Library used: `sklearn.ensemble.RandomForestClassifier`\n",
    "\n",
    "### 7.2.5 Gradient Boostingï¼ˆæ¢¯åº¦æå‡ï¼‰\n",
    "\n",
    "æ¢¯åº¦æå‡ç»“åˆå¤šä¸ªå¼±å­¦ä¹ å™¨ï¼Œæ¯ä¸ªæ¨¡å‹ä¿®æ­£å‰ä¸€ä¸ªæ¨¡å‹çš„é”™è¯¯ã€‚  \n",
    "Gradient Boosting combines weak learners, with each new model correcting the previous one's errors.\n",
    "\n",
    "- è¡¨ç°å¼ºå¤§ï¼Œå¯å¤„ç†ç¼ºå¤±å€¼ä¸å¼‚å¸¸å€¼  \n",
    "  Powerful performance; handles missing data and outliers\n",
    "- æ˜“è¿‡æ‹Ÿåˆï¼Œè®¡ç®—æˆæœ¬è¾ƒé«˜  \n",
    "  Susceptible to overfitting; computationally expensive\n",
    "- ä½¿ç”¨åº“ï¼š`sklearn.ensemble.GradientBoostingClassifier`  \n",
    "  Library used: `sklearn.ensemble.GradientBoostingClassifier`\n",
    "\n",
    "\n",
    "## 7.3 Evaluation Metricsï¼ˆè¯„ä¼°æŒ‡æ ‡ï¼‰\n",
    "\n",
    "### 7.3.1 æ··æ·†çŸ©é˜µï¼ˆConfusion Matrixï¼‰\n",
    "\n",
    "æ··æ·†çŸ©é˜µç”¨äºè¯„ä¼°åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ã€‚  \n",
    "The confusion matrix evaluates classification model performance.\n",
    "\n",
    "| Term | ä¸­æ–‡è§£é‡Š | English Definition |\n",
    "|------|----------|---------------------|\n",
    "| TP | çœŸé˜³æ€§ï¼šæ­£ç¡®é¢„æµ‹ä¸ºæ­£ç±» | True Positives: Correctly predicted positives |\n",
    "| TN | çœŸé˜´æ€§ï¼šæ­£ç¡®é¢„æµ‹ä¸ºè´Ÿç±» | True Negatives: Correctly predicted negatives |\n",
    "| FP | å‡é˜³æ€§ï¼šé”™è¯¯é¢„æµ‹ä¸ºæ­£ç±» | False Positives: Incorrectly predicted positives |\n",
    "| FN | å‡é˜´æ€§ï¼šé”™è¯¯é¢„æµ‹ä¸ºè´Ÿç±» | False Negatives: Incorrectly predicted negatives |\n",
    "\n",
    "### 7.3.2 ä¸»è¦æŒ‡æ ‡ï¼ˆKey Metricsï¼‰\n",
    "\n",
    "- **å‡†ç¡®ç‡ Accuracy**ï¼šæ­£ç¡®é¢„æµ‹æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹  \n",
    "  Accuracy: Ratio of correct predictions over all samples\n",
    "- **ç²¾ç¡®ç‡ Precision**ï¼šé¢„æµ‹ä¸ºæ­£ç±»ä¸­çœŸæ­£ä¸ºæ­£ç±»çš„æ¯”ä¾‹  \n",
    "  Precision: TP / (TP + FP)\n",
    "- **å¬å›ç‡ Recall**ï¼šæ‰€æœ‰å®é™…æ­£ç±»ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹  \n",
    "  Recall: TP / (TP + FN)\n",
    "- **F1 åˆ†æ•° F1-Score**ï¼šç²¾ç¡®ç‡ä¸å¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼  \n",
    "  F1-Score: Harmonic mean of precision and recall\n",
    "- æ¨èä½¿ç”¨ `sklearn.metrics.classification_report` è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š  \n",
    "  Recommended: `sklearn.metrics.classification_report` for auto reporting\n",
    "\n",
    "### 7.3.3 é˜ˆå€¼è°ƒæ•´ï¼ˆThreshold Adjustmentï¼‰\n",
    "\n",
    "- é»˜è®¤é˜ˆå€¼ä¸º 0.5ï¼Œå¯è°ƒæ•´ä»¥æ”¹å˜å¬å›ç‡ä¸ç²¾ç¡®ç‡çš„å¹³è¡¡  \n",
    "  Default threshold is 0.5; changing it adjusts recall vs. precision trade-off\n",
    "- ä¾‹å¦‚é™ä½é˜ˆå€¼å¯æé«˜å¬å›ç‡ï¼Œä½†å¢åŠ è¯¯æŠ¥ç‡  \n",
    "  Lowering the threshold raises recall but also increases false positives\n",
    "\n",
    "### 7.3.4 ROC-AUC\n",
    "\n",
    "- ROC æ›²çº¿æ˜¾ç¤ºä¸åŒé˜ˆå€¼ä¸‹çš„ TPRï¼ˆå¬å›ç‡ï¼‰å’Œ FPRï¼ˆå‡é˜³æ€§ç‡ï¼‰  \n",
    "  ROC curve plots TPR vs. FPR at different thresholds\n",
    "- AUC è¡¨ç¤ºæ¨¡å‹åŒºåˆ†æ­£è´Ÿæ ·æœ¬çš„èƒ½åŠ›  \n",
    "  AUC measures how well the model separates classes\n",
    "- AUC = 1 è¡¨ç¤ºå®Œç¾åˆ†ç±»ï¼ŒAUC = 0.5 è¡¨ç¤ºéšæœºçŒœæµ‹  \n",
    "  AUC = 1 is perfect; AUC = 0.5 equals random guessing\n",
    "\n",
    "\n",
    "## 7.4 Imbalanced Classesï¼ˆç±»åˆ«ä¸å¹³è¡¡ï¼‰\n",
    "\n",
    "### 7.4.1 é—®é¢˜æè¿°ï¼ˆProblem Descriptionï¼‰\n",
    "\n",
    "åœ¨çœŸå®æ•°æ®ä¸­ï¼Œå¸¸å¸¸å‡ºç°æŸä¸€ç±»åˆ«æ ·æœ¬æ•°é‡è¿œè¿œå¤§äºå…¶ä»–ç±»åˆ«çš„æƒ…å†µï¼Œä¾‹å¦‚æ¬ºè¯ˆæ£€æµ‹ã€åƒåœ¾é‚®ä»¶è¯†åˆ«ã€‚  \n",
    "In real-world datasets, some classes (e.g., fraud) are much rarer than others (non-fraud).\n",
    "\n",
    "è¿™ä¼šå¯¼è‡´æ¨¡å‹åå‘å¤šæ•°ç±»ï¼Œå°¤å…¶å½±å“å‡†ç¡®ç‡ç­‰è¯„ä¼°æŒ‡æ ‡ã€‚  \n",
    "This leads to bias toward the majority class, especially affecting accuracy.\n",
    "\n",
    "### 7.4.2 å¤„ç†æ–¹æ³•ï¼ˆHandling Methodsï¼‰\n",
    "\n",
    "- æ”¶é›†æ›´å¤šæ•°æ®  \n",
    "  Collect more data\n",
    "- é‡é‡‡æ ·ï¼ˆResamplingï¼‰  \n",
    "  Resampling: either oversample minority or undersample majority\n",
    "- ä½¿ç”¨é›†æˆæ–¹æ³•ï¼ˆå¦‚ Random Forest æˆ– Gradient Boostingï¼‰  \n",
    "  Use ensemble methods like Random Forest or Gradient Boosting\n",
    "- åœ¨æç«¯ä¸å¹³è¡¡æƒ…å†µä¸‹ä½¿ç”¨å¼‚å¸¸æ£€æµ‹æ¨¡å‹  \n",
    "  Use anomaly detection for extreme imbalance\n",
    "\n",
    "### 7.4.3 é‡é‡‡æ ·æŠ€æœ¯ï¼ˆResampling Techniquesï¼‰\n",
    "\n",
    "- **éšæœºæ¬ é‡‡æ ·ï¼ˆRandom Undersamplingï¼‰**ï¼šéšæœºä¸¢å¼ƒå¤šæ•°ç±»æ ·æœ¬  \n",
    "  Randomly discard majority class samples\n",
    "- **SMOTEï¼ˆåˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·ï¼‰**ï¼šæ ¹æ®æœ€è¿‘é‚»åˆ›å»ºå°‘æ•°ç±»åˆæˆæ ·æœ¬  \n",
    "  SMOTE: Synthetic Minority Oversampling Technique creates synthetic minority samples\n",
    "\n",
    "æ³¨æ„ï¼šåªåº”åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œé‡é‡‡æ ·ï¼Œæµ‹è¯•é›†ä¿æŒåŸåˆ†å¸ƒã€‚  \n",
    "Note: Apply resampling only to the training set to preserve true test distribution.\n",
    "\n",
    "\n",
    "## 7.5 Train-Test Splitï¼ˆè®­ç»ƒ-æµ‹è¯•åˆ’åˆ†ï¼‰\n",
    "\n",
    "ä¸ºäº†ä¿æŒè¯„ä¼°æŒ‡æ ‡çš„ç¨³å®šæ€§ï¼Œåº”é‡‡ç”¨åˆ†å±‚é‡‡æ ·ã€‚  \n",
    "To ensure stable evaluation, use stratified sampling.\n",
    "\n",
    "- `train_test_split()` ä¸­ä½¿ç”¨ `stratify` å‚æ•°  \n",
    "  Use `stratify` parameter in `train_test_split()`\n",
    "- `StratifiedKFold` ç”¨äºåˆ†å±‚äº¤å‰éªŒè¯  \n",
    "  Use `StratifiedKFold` for stratified cross-validation\n",
    "- ä¿è¯æ¯ä¸ªç±»åˆ«åœ¨åˆ’åˆ†ä¸­æŒ‰æ¯”ä¾‹å‡ºç°  \n",
    "  Ensures each class is proportionally represented in each fold\n",
    "\n",
    "\n",
    "## 7.6 Metric Selection Scenariosï¼ˆæŒ‡æ ‡é€‰æ‹©æ¡ˆä¾‹ï¼‰\n",
    "\n",
    "- **ä»»åŠ¡1ï¼šè¯†åˆ«æ˜¯å¦ä¸­æ¯’æ¤ç‰©**  \n",
    "  ä¼˜å…ˆè€ƒè™‘å¬å›ç‡ Recallï¼ˆä¸è¦æ¼æ‰æ¯’æ€§æ¤ç‰©ï¼‰  \n",
    "  Task: Identify poisonous plants â†’ focus on **Recall**\n",
    "\n",
    "- **ä»»åŠ¡2ï¼šé¢„æµ‹æ˜‚è´µçš„æ²»ç–—æ˜¯å¦éœ€è¦**  \n",
    "  ä¼˜å…ˆè€ƒè™‘å¬å›ç‡ Recallï¼ˆå®å¯è¯¯åˆ¤ä¹Ÿä¸èƒ½æ¼è¯Šï¼‰  \n",
    "  Task: Predict need for expensive therapy â†’ focus on **Recall**\n",
    "\n",
    "- **ä»»åŠ¡3ï¼šè¯†åˆ«ç‹—çš„å“ç§ï¼ˆ100ç§å‡åŒ€åˆ†å¸ƒï¼‰**  \n",
    "  å¤šç±»å¹³è¡¡ â†’ å¯ç”¨å‡†ç¡®ç‡ Accuracy æˆ– F1 åˆ†æ•° F1-score  \n",
    "  Task: Predict dog breed â†’ suitable metrics: **Accuracy** or **F1-score**\n",
    "\n",
    "## 7.7 Summaryï¼ˆæ€»ç»“ï¼‰\n",
    "\n",
    "åˆ†ç±»å»ºæ¨¡æ˜¯æ•°æ®ç§‘å­¦ä¸­é‡è¦çš„ä¸€ç¯ï¼Œæ¶‰åŠé€‰æ‹©é€‚å½“æ¨¡å‹ã€è¯„ä¼°æŒ‡æ ‡ã€å¤„ç†ä¸å¹³è¡¡ã€è®¾ç½®é˜ˆå€¼ç­‰å¤šä¸ªæ–¹é¢ã€‚  \n",
    "Classification modeling is a crucial step in data science, involving model selection, evaluation, class imbalance handling, and threshold tuning.\n",
    "\n",
    "åˆç†é€‰æ‹©æŒ‡æ ‡å’ŒæŠ€æœ¯ï¼Œå¯ä»¥æå‡æ¨¡å‹çš„å®ç”¨æ€§ä¸è§£é‡Šæ€§ã€‚  \n",
    "Proper selection of metrics and techniques improves both utility and interpretability of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19574cb4",
   "metadata": {},
   "source": [
    "# 8. Unsupervised Learning & Optimizationï¼ˆæ— ç›‘ç£å­¦ä¹ ä¸ä¼˜åŒ–ï¼‰\n",
    "\n",
    "æœ¬ç« ä¸»è¦æ¢è®¨æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯èšç±»ï¼‰ä»¥åŠå¦‚ä½•é€šè¿‡è¶…å‚æ•°è°ƒä¼˜æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå¹¶è§£é‡Šåå·®-æ–¹å·®æƒè¡¡ã€è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆé—®é¢˜ã€‚  \n",
    "This chapter focuses on unsupervised learning methods (especially clustering) and how to optimize model performance through hyperparameter tuning, explaining bias-variance trade-off, overfitting, and underfitting.\n",
    "\n",
    "## 8.1 æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰\n",
    "\n",
    "æ— ç›‘ç£å­¦ä¹ ç”¨äºæ— æ ‡ç­¾æ•°æ®ï¼Œç›®æ ‡æ˜¯åŸºäºæ ·æœ¬é—´çš„ç›¸ä¼¼æ€§å‘ç°æ•°æ®ç»“æ„æˆ–æ¨¡å¼ã€‚  \n",
    "Unsupervised learning deals with unlabeled data, aiming to discover structure or patterns based on similarities among samples.\n",
    "\n",
    "å¸¸è§åº”ç”¨åŒ…æ‹¬ï¼š  \n",
    "Common applications include:\n",
    "\n",
    "- æ–‡æ¡£æˆ–æ–‡ç« çš„åˆ†ç»„  \n",
    "  Grouping similar documents or articles\n",
    "- å®¢æˆ·åˆ†ç¾¤ï¼ˆä¾æ®è´­ä¹°è¡Œä¸ºï¼‰  \n",
    "  Customer segmentation based on purchasing behavior\n",
    "- ç”Ÿç‰©ç‰©ç§èšç±»ï¼ˆä¾æ®é—ä¼ ç‰¹å¾ï¼‰  \n",
    "  Grouping species based on genetic similarities\n",
    "\n",
    "é€šå¸¸åŸºäºæ¬§å‡ é‡Œå¾—è·ç¦»ç­‰è·ç¦»åº¦é‡ï¼Œè§£é‡Šæ€§è¾ƒä½ã€‚  \n",
    "Usually based on distance metrics such as Euclidean distance; low interpretability.\n",
    "\n",
    "## 8.2 èšç±»æ–¹æ³•ï¼ˆClustering Methodsï¼‰\n",
    "\n",
    "èšç±»æ–¹æ³•æ ¹æ®æ ·æœ¬é—´çš„è·ç¦»æˆ–å¯†åº¦å°†æ•°æ®ç‚¹å½’ä¸ºä¸€ç»„ã€‚  \n",
    "Clustering methods group data points based on distance or density.\n",
    "\n",
    "- **åŸºäºä¸­å¿ƒç‚¹ï¼ˆCentroid-basedï¼‰**ï¼šå¦‚ KMeans  \n",
    "  Centroid-based (e.g., KMeans)\n",
    "- **åŸºäºè¿æ¥æ€§ï¼ˆHierarchical/Connectivity-basedï¼‰**ï¼šå¦‚å‡èšå±‚æ¬¡èšç±»  \n",
    "  Connectivity-based (e.g., Agglomerative Clustering)\n",
    "- **åŸºäºå¯†åº¦ï¼ˆDensity-basedï¼‰**ï¼šå¦‚ DBSCAN  \n",
    "  Density-based (e.g., DBSCAN)\n",
    "- **åŸºäºå›¾ç»“æ„ï¼ˆGraph-basedï¼‰**\n",
    "- **åŸºäºåˆ†å¸ƒï¼ˆDistribution-basedï¼‰**\n",
    "\n",
    "\n",
    "## 8.3 KMeans èšç±»ï¼ˆKMeans Clusteringï¼‰\n",
    "\n",
    "KMeans æ˜¯æœ€å¸¸è§çš„èšç±»ç®—æ³•ï¼Œéœ€é¢„å…ˆè®¾å®šèšç±»æ•° *k*ã€‚  \n",
    "KMeans is one of the most commonly used clustering algorithms and requires a predefined number of clusters *k*.\n",
    "\n",
    "æµç¨‹å¦‚ä¸‹ï¼š  \n",
    "Steps:\n",
    "\n",
    "1. éšæœºé€‰æ‹© *k* ä¸ªåˆå§‹è´¨å¿ƒ  \n",
    "   Randomly pick *k* centroids\n",
    "2. å°†æ¯ä¸ªæ ·æœ¬åˆ†é…ç»™æœ€è¿‘çš„è´¨å¿ƒ  \n",
    "   Assign each point to the nearest centroid\n",
    "3. é‡æ–°è®¡ç®—æ¯ä¸ªè´¨å¿ƒä¸ºè¯¥ç»„ç‚¹çš„å‡å€¼  \n",
    "   Recalculate centroids as the mean of assigned points\n",
    "4. é‡å¤ç›´åˆ°è´¨å¿ƒç¨³å®šï¼Œæœ€å°åŒ–ç»„å†…å˜å¼‚ï¼ˆWSSï¼‰  \n",
    "   Repeat until convergence minimizing Within-Cluster Sum of Squares (WSS)\n",
    "\n",
    "ç‰¹ç‚¹ï¼šå¯¹å¼‚å¸¸å€¼å’Œæ•°æ®ç¼©æ”¾æ•æ„Ÿï¼›éœ€è¦é¢„å®šä¹‰ *k* å€¼ã€‚  \n",
    "Sensitive to outliers and feature scaling; requires predefined *k*.\n",
    "\n",
    "ä½¿ç”¨åº“ä¸å‡½æ•°ï¼š`sklearn.cluster.KMeans`  \n",
    "Library: `sklearn.cluster.KMeans`\n",
    "\n",
    "\n",
    "### 8.3.1 è¯„ä¼°èšç±»æ•°ï¼šè‚˜éƒ¨æ³•ï¼ˆElbow Methodï¼‰\n",
    "\n",
    "ç”¨äºç¡®å®šåˆé€‚çš„ *k* å€¼ã€‚  \n",
    "Used to determine an appropriate number of clusters *k*.\n",
    "\n",
    "- ç»˜åˆ¶ä¸åŒ *k* å¯¹åº”çš„ WSSï¼ˆç»„å†…å¹³æ–¹å’Œï¼‰  \n",
    "  Plot WSS vs. *k*\n",
    "- å¯»æ‰¾â€œè‚˜éƒ¨â€æ‹ç‚¹ï¼Œä½œä¸ºå»ºè®®èšç±»æ•°  \n",
    "  Locate the \"elbow\" point where WSS stops significantly decreasing\n",
    "\n",
    "\n",
    "### 8.3.2 èšç±»è´¨é‡è¯„ä¼°ï¼šè½®å»“ç³»æ•°ï¼ˆSilhouette Scoreï¼‰\n",
    "\n",
    "- è¡¡é‡æ¯ä¸ªç‚¹ä¸è‡ªèº«ç°‡å†…æ ·æœ¬çš„ç´§å¯†åº¦ä»¥åŠä¸å…¶ä»–ç°‡çš„åˆ†ç¦»åº¦  \n",
    "  Measures how similar a point is to its own cluster compared to other clusters\n",
    "- å€¼èŒƒå›´ [-1, 1]ï¼Œè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼Œ0 è¡¨ç¤ºé‡å ï¼Œè´Ÿå€¼è¡¨ç¤ºå¯èƒ½é”™è¯¯å½’ç±»  \n",
    "  Closer to 1 is better, 0 means overlapping, negative means likely misassigned\n",
    "\n",
    "ä½¿ç”¨å‡½æ•°ï¼š`sklearn.metrics.silhouette_score`  \n",
    "Function: `sklearn.metrics.silhouette_score(X, labels)`\n",
    "\n",
    "\n",
    "\n",
    "## 8.4 åˆ†å±‚èšç±»ï¼ˆHierarchical Clusteringï¼‰\n",
    "\n",
    "æä¾›è‡ªä¸Šè€Œä¸‹æˆ–è‡ªä¸‹è€Œä¸Šçš„èšç±»ç»“æ„ã€‚  \n",
    "Provides top-down or bottom-up clustering structures.\n",
    "\n",
    "- **å‡èšæ³•ï¼ˆAgglomerativeï¼‰**ï¼šè‡ªåº•å‘ä¸Šï¼Œæ¯ä¸ªç‚¹ä¸ºå•ç‹¬ç°‡ï¼Œé€æ­¥åˆå¹¶  \n",
    "  Bottom-up: start from individual points, merge based on similarity\n",
    "- **åˆ†è£‚æ³•ï¼ˆDivisiveï¼‰**ï¼šè‡ªé¡¶å‘ä¸‹ï¼Œä»æ•´ä½“é€æ­¥åˆ†è£‚  \n",
    "  Top-down: start from all data in one cluster, split based on dissimilarity\n",
    "\n",
    "ç»“æœå¯ç”¨æ ‘çŠ¶å›¾ï¼ˆDendrogramï¼‰è¡¨ç¤ºï¼Œåˆ‡æ–­æŸä¸€é«˜åº¦å†³å®šç°‡æ•°ã€‚  \n",
    "Represented as a dendrogram; cut at a certain height to determine number of clusters.\n",
    "\n",
    "\n",
    "# 9. Overfitting & Underfittingï¼ˆè¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆï¼‰\n",
    "\n",
    "\n",
    "## 9.1 ç°è±¡å®šä¹‰ï¼ˆDefinitionsï¼‰\n",
    "\n",
    "- **è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰**ï¼šæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è¿‡å¥½ï¼Œä½†æ³›åŒ–èƒ½åŠ›å·®  \n",
    "  Model performs well on training data but poorly on new (test) data\n",
    "- **æ¬ æ‹Ÿåˆï¼ˆUnderfittingï¼‰**ï¼šæ¨¡å‹æ— æ³•æ•æ‰æ•°æ®ä¸­çš„æ¨¡å¼  \n",
    "  Model fails to capture underlying patterns in the data\n",
    "\n",
    "è¯†åˆ«æ–¹æ³•ï¼š  \n",
    "How to identify:\n",
    "\n",
    "- è¿‡æ‹Ÿåˆï¼šè®­ç»ƒè¯¯å·®ä½ï¼Œæµ‹è¯•è¯¯å·®é«˜  \n",
    "  Overfitting: Low training error, high test error\n",
    "- æ¬ æ‹Ÿåˆï¼šè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®éƒ½é«˜  \n",
    "  Underfitting: High error on both training and test sets\n",
    "\n",
    "\n",
    "## 9.2 åå·®-æ–¹å·®æƒè¡¡ï¼ˆBias-Variance Trade-offï¼‰\n",
    "\n",
    "- **åå·®ï¼ˆBiasï¼‰**ï¼šæ¨¡å‹é¢„æµ‹çš„å¹³å‡å€¼ä¸çœŸå®å€¼çš„å·®å¼‚  \n",
    "  Bias: Difference between predicted and true values\n",
    "- **æ–¹å·®ï¼ˆVarianceï¼‰**ï¼šæ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„é¢„æµ‹æ³¢åŠ¨æ€§  \n",
    "  Variance: Variability of predictions across datasets\n",
    "\n",
    "- é«˜åå·®ã€ä½æ–¹å·® â†’ æ¬ æ‹Ÿåˆï¼ˆå¦‚çº¿æ€§æ¨¡å‹ï¼‰  \n",
    "  High bias, low variance â†’ Underfitting (e.g., linear models)\n",
    "- ä½åå·®ã€é«˜æ–¹å·® â†’ è¿‡æ‹Ÿåˆï¼ˆå¦‚æ·±å±‚å†³ç­–æ ‘ã€kNNï¼‰  \n",
    "  Low bias, high variance â†’ Overfitting (e.g., deep decision trees, kNN)\n",
    "\n",
    "\n",
    "## 9.3 è¿‡æ‹ŸåˆåŸå› ä¸è§£å†³æ–¹æ¡ˆï¼ˆCauses & Solutionsï¼‰\n",
    "\n",
    "### åŸå› ï¼ˆCausesï¼‰\n",
    "\n",
    "- è®­ç»ƒé›†å¤ªå°  \n",
    "  Training set too small\n",
    "- æ•°æ®å™ªå£°è¿‡å¤š  \n",
    "  Too much noise in data\n",
    "- è®­ç»ƒè½®æ¬¡è¿‡å¤š  \n",
    "  Trained too long (especially in deep learning)\n",
    "- æ¨¡å‹å¤æ‚åº¦è¿‡é«˜  \n",
    "  Model too complex\n",
    "\n",
    "### è§£å†³æ–¹æ¡ˆï¼ˆFixesï¼‰\n",
    "\n",
    "- å¢åŠ è®­ç»ƒæ•°æ®é‡  \n",
    "  Increase training set size\n",
    "- æ¸…æ´—æ•°æ®ï¼ˆå¤„ç†å¼‚å¸¸å€¼å’Œç¼ºå¤±å€¼ï¼‰  \n",
    "  Clean data (handle outliers and missing values)\n",
    "- æå‰åœæ­¢è®­ç»ƒï¼ˆearly stoppingï¼‰  \n",
    "  Early stopping\n",
    "- æ­£åˆ™åŒ–ï¼ˆRegularizationï¼Œå¦‚ L1/L2ï¼‰  \n",
    "  Regularization (e.g., L1, L2)\n",
    "- é™ç»´ï¼ˆå»é™¤æ— å…³æˆ–å¼±ç›¸å…³ç‰¹å¾ï¼‰  \n",
    "  Dimensionality reduction\n",
    "- è¶…å‚æ•°è°ƒä¼˜  \n",
    "  Hyperparameter tuning\n",
    "\n",
    "\n",
    "# 10. Hyperparameter Tuningï¼ˆè¶…å‚æ•°è°ƒä¼˜ï¼‰\n",
    "\n",
    "## 10.1 æ¦‚è¿°ï¼ˆOverviewï¼‰\n",
    "\n",
    "- **å‚æ•°ï¼ˆParametersï¼‰**ï¼šæ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ çš„å€¼  \n",
    "  Parameters: learned from data during training\n",
    "- **è¶…å‚æ•°ï¼ˆHyperparametersï¼‰**ï¼šæ¨¡å‹ç»“æ„/è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¾ç½®ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ ‘æ·±ï¼‰  \n",
    "  Hyperparameters: preset values controlling model structure or training\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦è°ƒä¼˜ï¼Ÿ  \n",
    "Why tune?\n",
    "\n",
    "- æ‰¾åˆ°æ¨¡å‹è¡¨ç°æœ€å¥½çš„è®¾ç½®ç»„åˆ  \n",
    "  To find the best-performing configuration\n",
    "- ä»£ä»·é«˜ï¼Œä½†èƒ½æ˜¾è‘—æå‡æ€§èƒ½  \n",
    "  Computationally expensive, but effective\n",
    "\n",
    "\n",
    "## 10.2 è°ƒä¼˜æ–¹æ³•ï¼ˆTuning Methodsï¼‰\n",
    "\n",
    "### ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰\n",
    "\n",
    "- ç©·ä¸¾æœç´¢æ‰€æœ‰è¶…å‚æ•°ç»„åˆ  \n",
    "  Exhaustive search over all combinations\n",
    "- ä½¿ç”¨å‡½æ•°ï¼š`sklearn.model_selection.GridSearchCV`  \n",
    "  Function: `sklearn.model_selection.GridSearchCV`\n",
    "\n",
    "### éšæœºæœç´¢ï¼ˆRandom Searchï¼‰\n",
    "\n",
    "- éšæœºé‡‡æ ·è¶…å‚æ•°ç©ºé—´  \n",
    "  Random sampling of hyperparameter space\n",
    "- æ•ˆç‡æ›´é«˜ä½†å¯èƒ½é”™è¿‡æœ€ä¼˜è§£  \n",
    "  More efficient, but may miss optimal combination\n",
    "\n",
    "\n",
    "## 10.3 ç¤ºä¾‹è°ƒä¼˜ç©ºé—´ï¼ˆExample Search Spaceï¼‰\n",
    "\n",
    "```python\n",
    "gr_space = {\n",
    "  'max_depth': [3,5,7,10],\n",
    "  'n_estimators': [100, 200, 300, 400, 500],\n",
    "  'max_features': [10, 20, 30, 40],\n",
    "  'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "```\n",
    "\n",
    "## 10.4 K æŠ˜äº¤å‰éªŒè¯ï¼ˆK-Fold Cross-Validationï¼‰\n",
    "\n",
    "ç”¨äºè¯„ä¼°è°ƒä¼˜è¿‡ç¨‹ä¸­æ¯ç§å‚æ•°ç»„åˆçš„è¡¨ç°ã€‚  \n",
    "Used to evaluate model performance across hyperparameter configurations.\n",
    "\n",
    "- å°†è®­ç»ƒé›†åˆ’åˆ†ä¸º K ä¸ªå­é›†ï¼ˆfoldsï¼‰  \n",
    "  Split training data into K subsets\n",
    "- æ¯æ¬¡é€‰ä¸€ä¸ªå­é›†ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä½™ç”¨äºè®­ç»ƒ  \n",
    "  Use one fold for validation and others for training\n",
    "- è®¡ç®—æ‰€æœ‰ K æ¬¡ç»“æœçš„å¹³å‡å€¼  \n",
    "  Average the results across folds\n",
    "\n",
    "ä½¿ç”¨åº“ï¼š`sklearn.model_selection.KFold` æˆ– `StratifiedKFold`  \n",
    "Libraries: `sklearn.model_selection.KFold` or `StratifiedKFold`\n",
    "\n",
    "\n",
    "# 11. Feature Importanceï¼ˆç‰¹å¾é‡è¦æ€§ï¼‰\n",
    "\n",
    "è®­ç»ƒå®Œæˆçš„æ¨¡å‹å¯ä»¥ç”¨äºè¯„ä¼°å“ªäº›ç‰¹å¾å¯¹æœ€ç»ˆé¢„æµ‹ç»“æœæœ€ä¸ºå…³é”®ã€‚  \n",
    "Trained models can reveal which features are most important for prediction.\n",
    "\n",
    "- ç‰¹å¾é‡è¦æ€§å¯ç”¨äºç®€åŒ–æ¨¡å‹æˆ–è§£é‡Šæ¨¡å‹è¡Œä¸º  \n",
    "  Feature importance can help simplify the model or interpret decisions\n",
    "- ä¸åŒæ¨¡å‹è¯„ä¼°æ–¹æ³•ä¸åŒï¼Œå¦‚æ ‘æ¨¡å‹é€šå¸¸åŸºäºä¿¡æ¯å¢ç›Š/ä¸çº¯åº¦ä¸‹é™  \n",
    "  Different models measure importance differently (e.g., tree models use impurity decrease)\n",
    "- åœ¨ä½¿ç”¨éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰æˆ–æ¢¯åº¦æå‡æ ‘ï¼ˆGradient Boostingï¼‰æ—¶ï¼Œå¯ç›´æ¥æŸ¥çœ‹ç‰¹å¾é‡è¦æ€§å±æ€§  \n",
    "  For models like Random Forest or Gradient Boosting, feature importance can be directly retrieved\n",
    "\n",
    "ä½¿ç”¨å‡½æ•°ï¼š`model.feature_importances_`  \n",
    "Function: `model.feature_importances_`\n",
    "\n",
    "# æ€»ç»“ï¼ˆSummaryï¼‰\n",
    "\n",
    "- æ— ç›‘ç£å­¦ä¹ å¯ç”¨äºæ¢ç´¢æ•°æ®ç»“æ„ï¼ˆå¦‚èšç±»ï¼‰  \n",
    "  Unsupervised learning is used to explore structure (e.g., clustering)\n",
    "\n",
    "- èšç±»æ–¹æ³•åŒ…æ‹¬ KMeansã€å±‚æ¬¡èšç±»ã€å¯†åº¦èšç±»ç­‰ï¼›è¯„ä¼°æŒ‡æ ‡æœ‰ WSSã€è½®å»“ç³»æ•°ç­‰  \n",
    "  Clustering methods include KMeans, hierarchical, and density-based; metrics include WSS and silhouette score\n",
    "\n",
    "- æ¨¡å‹è®­ç»ƒéœ€é¿å…è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆï¼Œå…³é”®åœ¨äºç†è§£åå·®-æ–¹å·®æƒè¡¡  \n",
    "  Training should avoid overfitting/underfitting by understanding the bias-variance trade-off\n",
    "\n",
    "- é€ æˆè¿‡æ‹Ÿåˆçš„åŸå› åŒ…æ‹¬è®­ç»ƒé›†è¿‡å°ã€å™ªå£°å¤ªå¤šã€è®­ç»ƒæ—¶é—´è¿‡é•¿ã€æ¨¡å‹å¤æ‚åº¦è¿‡é«˜  \n",
    "  Overfitting may be caused by small datasets, noisy data, too long training, or overly complex models\n",
    "\n",
    "- è¶…å‚æ•°è°ƒä¼˜ï¼ˆGridSearch æˆ– RandomSearchï¼‰ä¸äº¤å‰éªŒè¯å¯ç”¨äºå¯»æ‰¾æœ€ä½³æ¨¡å‹é…ç½®  \n",
    "  Hyperparameter tuning (GridSearch or RandomSearch) and cross-validation help find the best model configuration\n",
    "\n",
    "- ç‰¹å¾é‡è¦æ€§åˆ†ææœ‰åŠ©äºæ¨¡å‹è§£é‡Šä¸ç®€åŒ–  \n",
    "  Feature importance helps with model interpretation and simplification\n",
    "\n",
    "- å®è·µä¸­è¯·è®°å¾—ï¼šé«˜å‡†ç¡®ç‡ä¸ç­‰äºå¥½æ¨¡å‹ï¼Œè‰¯å¥½æ³›åŒ–èƒ½åŠ›æ‰æ˜¯å…³é”®  \n",
    "  Remember: High training accuracy â‰  good model; generalization is key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a85fa2",
   "metadata": {},
   "source": [
    "# 12. Evaluation & Recommendationsï¼ˆæ¨¡å‹è¯„ä¼°ä¸å»ºè®®ï¼‰\n",
    "\n",
    "è¯„ä¼°æ˜¯ CRISP-DM æµç¨‹çš„æœ€åé˜¶æ®µï¼Œé‡ç‚¹åœ¨äºè§£é‡Šæ¨¡å‹ç»“æœã€åˆ†æå…¶æ„ä¹‰å¹¶æ®æ­¤æå‡ºå…·ä½“å»ºè®®ã€‚  \n",
    "Evaluation is the final phase of the CRISP-DM process, focusing on interpreting results, analyzing their impact, and providing actionable recommendations.\n",
    "\n",
    "\n",
    "## 12.1 è¯„ä¼°é˜¶æ®µç›®æ ‡ï¼ˆGoal of Evaluation Phaseï¼‰\n",
    "\n",
    "åœ¨è¯„ä¼°é˜¶æ®µï¼Œä½ éœ€è¦å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š  \n",
    "During the evaluation phase, answer:\n",
    "\n",
    "- ä½ çš„æ¨¡å‹ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ  \n",
    "  What are your results?\n",
    "- è¿™äº›ç»“æœæ„å‘³ç€ä»€ä¹ˆï¼Ÿå¯¹ä¸šåŠ¡æœ‰ä½•å½±å“ï¼Ÿ  \n",
    "  What do they mean and what is their impact?\n",
    "- æ¨¡å‹æ˜¯å¦å®ç°äº†é¢„æœŸä¸šåŠ¡ç›®æ ‡ï¼Ÿ  \n",
    "  Did the model achieve the business objectives?\n",
    "- æ¥ä¸‹æ¥åº”è¯¥åšä»€ä¹ˆï¼Ÿ  \n",
    "  What are the next steps?\n",
    "\n",
    "\n",
    "## 12.2 æ¼æ–—å¼ç»“æ„ï¼ˆHourglass Methodï¼‰\n",
    "\n",
    "- åœ¨ç»“è®ºéƒ¨åˆ†ä»å…·ä½“æ¨¡å‹ç»“æœé€æ­¥â€œæ”¾å¤§â€åˆ°æ•´ä¸ªé¡¹ç›®ç›®æ ‡  \n",
    "  In the conclusion, zoom out from the specific results to the overall objectives\n",
    "- æ˜ç¡®åœ°å°†ç»“æœä¸æœ€åˆçš„å‡è®¾æˆ–ä¸šåŠ¡ç›®æ ‡è”ç³»èµ·æ¥  \n",
    "  Explicitly connect results to original hypotheses or business goals\n",
    "- åæ€å»ºæ¨¡è¿‡ç¨‹ä¸­åšå‡ºçš„é€‰æ‹©åŠå…¶å½±å“  \n",
    "  Reflect on modeling choices and their impact\n",
    "\n",
    "\n",
    "## 12.3 è§£é‡ŠæŒ‡æ ‡å«ä¹‰ï¼ˆInterpreting Metricsï¼‰\n",
    "\n",
    "è¯„ä¼°åº”åŸºäºå¯è§£é‡Šçš„æŒ‡æ ‡è¿›è¡Œï¼Œç”¨é€šä¿—è¯­è¨€è¯´æ˜å…¶å«ä¹‰ï¼š  \n",
    "Evaluation should be grounded in interpretable metrics, expressed in layman's terms:\n",
    "\n",
    "- **MAE = 0.6** â†’ å¹³å‡è¯¯å·®ä¸º 0.6 ä¸ªå•ä½  \n",
    "  MAE = 0.6 â†’ On average, predictions are off by 0.6 units\n",
    "- è¯„ä¼°å…¶æ˜¯å¦â€œå¥½â€éœ€ç»“åˆä¸Šä¸‹æ–‡ï¼ˆå¦‚å•ä½æ˜¯è¯ç‰©å‰‚é‡æ—¶ï¼Œ0.6 å¯èƒ½å¾ˆå¤§ï¼‰  \n",
    "  Whether this is good or bad depends on the unit and context (e.g., dosage in grams)\n",
    "\n",
    "\n",
    "## 12.4 å›é¡¾æ•°æ®å‡†å¤‡ï¼ˆReflect on Data Prepï¼‰\n",
    "\n",
    "åœ¨è¯„ä¼°æ¨¡å‹ç»“æœæ—¶ï¼Œä¹Ÿè¦åæ€ä»¥ä¸‹é—®é¢˜ï¼š  \n",
    "When evaluating results, also reflect on:\n",
    "\n",
    "- æ•°æ®ç†è§£/å‡†å¤‡ä¸­çš„æ­¥éª¤å¦‚ä½•å½±å“å»ºæ¨¡ï¼Ÿ  \n",
    "  How did your data understanding/preparation impact modeling?\n",
    "- æ˜¯å¦æœ‰å‰¯ä½œç”¨æˆ–é—æ¼çš„å› ç´ ï¼Ÿ  \n",
    "  Were there unintended side effects or overlooked factors?\n",
    "- ä½ ç°åœ¨çŸ¥é“çš„çŸ¥è¯†ï¼Œæ˜¯å¦èƒ½å¸®åŠ©ä½ åšå‡ºæ›´å¥½çš„å‡†å¤‡ï¼Ÿ  \n",
    "  Knowing what you know now, what would you have done differently?\n",
    "\n",
    "\n",
    "# 13. Recommendationsï¼ˆå»ºè®®ï¼‰\n",
    "\n",
    "## 13.1 å»ºè®®åº”å…·ä½“å¯æ‰§è¡Œï¼ˆActionable and Specificï¼‰\n",
    "\n",
    "å¥½çš„å»ºè®®åº”å…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š  \n",
    "Good recommendations should:\n",
    "\n",
    "- æ˜ç¡®é’ˆå¯¹é¡¹ç›®è¿‡ç¨‹åŠç»“æœ  \n",
    "  Be directly derived from project steps and results\n",
    "- è§£å†³åˆ©ç›Šç›¸å…³è€…çš„å®é™…é—®é¢˜  \n",
    "  Address stakeholder needs\n",
    "- åŒ…å«å®æ–½ç»†èŠ‚åŠæ•ˆæœè¯´æ˜  \n",
    "  Include how to implement and why it helps\n",
    "- ä¸åº”æ¨¡ç³Šã€æ³›æ³›è€Œè°ˆæˆ–è¶…å‡ºä½ çš„æ§åˆ¶èŒƒå›´  \n",
    "  Avoid vague generalities or issues outside your scope\n",
    "\n",
    "\n",
    "## 13.2 SMART å»ºè®®åŸåˆ™ï¼ˆSMART Recommendationsï¼‰\n",
    "\n",
    "æœ‰æ•ˆå»ºè®®åº”ç¬¦åˆ SMART åŸåˆ™ï¼š  \n",
    "Effective recommendations should follow SMART criteria:\n",
    "\n",
    "- **S**pecificï¼ˆå…·ä½“ï¼‰  \n",
    "- **M**easurableï¼ˆå¯è¡¡é‡ï¼‰  \n",
    "- **A**chievableï¼ˆå¯è¾¾æˆï¼‰  \n",
    "- **R**elevantï¼ˆç›¸å…³æ€§å¼ºï¼‰  \n",
    "- **T**ime-boundï¼ˆæœ‰æ—¶é—´çº¦æŸï¼‰\n",
    "\n",
    "ä¾‹å¦‚ï¼š  \n",
    "Example:\n",
    "\n",
    "- âŒâ€œæˆ‘ä»¬åº”è¯¥å°è¯•æ›´å¤šçš„æ•°æ®æŠ€æœ¯ã€‚â€  \n",
    "  \"We should try more data techniques.\" â†’ Too vague  \n",
    "- âœ…â€œå¦‚æœæˆ‘ä»¬åœ¨ä¸‹ä¸€é˜¶æ®µå¢åŠ æ¨¡å‹å¯¹ `battery_power` çš„ log è½¬æ¢ï¼Œå¯èƒ½æé«˜å›å½’æ¨¡å‹çš„ MAE å‡†ç¡®åº¦ã€‚â€  \n",
    "  \"Applying a log transformation to `battery_power` in the next phase could improve MAE.\" â†’ SMART!\n",
    "\n",
    "\n",
    "## 13.3 è¯šå®é¢å¯¹é¡¹ç›®ç¼ºé™·ï¼ˆHonest Self-Reflectionï¼‰\n",
    "\n",
    "åœ¨è¯„ä¼°å’Œå»ºè®®ä¸­ï¼Œä¿æŒè¯šå®æ˜¯å…³é”®ï¼š  \n",
    "Honesty is key in evaluation and recommendations:\n",
    "\n",
    "- æ‰¿è®¤é”™è¯¯ï¼Œå¹¶è¯´æ˜å¦‚ä½•é¿å…  \n",
    "  Acknowledge what went wrong and how to fix it\n",
    "- å±•ç¤ºä½ çš„åˆ†ææµç¨‹ï¼Œè€Œä¸ä»…ä»…æ˜¯ç»“æœ  \n",
    "  Show your process, not just the output\n",
    "- åšå‡ºåˆç†ã€å¯æ”¹è¿›çš„è¯„ä¼°ï¼Œè€Œä¸æ˜¯æ¨å¸è´£ä»»æˆ–çŒœæµ‹  \n",
    "  Offer reasonable, self-aware critique, not excuses\n",
    "\n",
    "\n",
    "# 14. Knowledge Check & Final Adviceï¼ˆè¯¾ç¨‹æµ‹è¯•ä¸æ€»ç»“ï¼‰\n",
    "\n",
    "## 14.1 æ¨¡æ‹Ÿæµ‹è¯•é¢˜ï¼ˆMock Questionsï¼‰\n",
    "\n",
    "1. **ä¸ºä»€ä¹ˆåªç”¨è®­ç»ƒé›†åšç‰¹å¾ç¼©æ”¾ï¼Ÿ**  \n",
    "   Why scale using only the training set?  \n",
    "   âœ… B. ä¸ºäº†æ¨¡æ‹Ÿæ¨¡å‹å¤„ç†â€œæœªè§æ•°æ®â€çš„æ–¹å¼ï¼ˆsimulate unseen dataï¼‰\n",
    "\n",
    "2. **å¦‚æœä¸šåŠ¡ç›®æ ‡æ˜¯å‡å°‘ä¸å¿…è¦çš„éšè®¿ï¼Œå“ªé¡¹æˆåŠŸæ ‡å‡†æœ€ç›¸å…³ï¼Ÿ**  \n",
    "   If the business goal is to reduce unnecessary follow-ups:  \n",
    "   âœ… C. é«˜ç²¾åº¦ï¼ˆHigh Precisionï¼‰\n",
    "\n",
    "3. **è‹¥ç‰¹å¾ä¸ºå³ååˆ†å¸ƒï¼Œå“ªç§å˜æ¢å¯ä»¥æå‡æ¨¡å‹æ€§èƒ½ï¼Ÿ**  \n",
    "   For right-skewed distributions:  \n",
    "   âœ… D. å¯¹æ•°å˜æ¢ï¼ˆLog transformï¼‰\n",
    "\n",
    "4. **ä¸ºä»€ä¹ˆåœ¨è¯„ä¼°å›å½’æ¨¡å‹æ—¶è€ƒè™‘ MAE è€Œä¸ä»…æ˜¯ MSEï¼Ÿ**  \n",
    "   Why consider MAE in addition to MSE?  \n",
    "   âœ… C. MAE å¯¹ä¸šåŠ¡åˆ©ç›Šç›¸å…³è€…æ›´æ˜“è§£é‡Šï¼ˆEasier to interpret for stakeholdersï¼‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
